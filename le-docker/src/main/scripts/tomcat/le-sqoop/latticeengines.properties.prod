# ==================================================
# le-common/conf/env/prodcluster/common.properties
# ==================================================
common.le.environment=prodcluster
common.le.stack=b

# for application runtime
common.admin.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com
common.microservice.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com
common.scoringapi.url=https://internal-public-lpi-b-556082394.us-east-1.elb.amazonaws.com
common.matchapi.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com
common.pls.url=https://internal-public-lpi-b-556082394.us-east-1.elb.amazonaws.com
common.playmaker.url=https://internal-public-lpi-b-556082394.us-east-1.elb.amazonaws.com
common.oauth.url=https://internal-public-lpi-b-556082394.us-east-1.elb.amazonaws.com
common.saml.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com
common.sqoop.url=https://10.51.12.101:8080
common.ulysses.url=https://internal-public-lpi-b-556082394.us-east-1.elb.amazonaws.com

common.adminconsole.url=https://admin.prod.lattice.local
common.quartz.stack.flag=false

common.allow.cors=false

# for deployment test client
common.test.env=prod

common.test.admin.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com
common.test.modeling.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com
common.test.microservice.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com
common.test.scoringapi.url=https://internal-public-lpi-b-556082394.us-east-1.elb.amazonaws.com
common.test.matchapi.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com
common.test.pls.url=https://internal-public-lpi-b-556082394.us-east-1.elb.amazonaws.com
common.test.playmaker.url=https://internal-public-lpi-b-556082394.us-east-1.elb.amazonaws.com
common.test.oauth.url=https://internal-public-lpi-b-556082394.us-east-1.elb.amazonaws.com
common.test.saml.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com
common.test.sqoop.url=https://10.51.12.101:8080
common.test.ulysses.url=https://internal-public-lpi-b-556082394.us-east-1.elb.amazonaws.com

# ==================================================
# le-workflow/conf/env/prodcluster/workflow.properties
# ==================================================
workflow.jobs.numthreads=20
workflow.jobs.cache.namespace=prodcluster

workflow.datasource.driver=com.mysql.jdbc.Driver
workflow.datasource.url=jdbc:mysql://lpi-encrypted-cluster.cluster-c6q8lwiagbkt.us-east-1.rds.amazonaws.com/PLS_MultiTenant?autoReconnect=true&useSSL=false
workflow.datasource.reader.url=jdbc:mysql://lpi-encrypted-cluster.cluster-ro-c6q8lwiagbkt.us-east-1.rds.amazonaws.com/PLS_MultiTenant?autoReconnect=true&useSSL=false
workflow.datasource.user=LPI
workflow.datasource.password.encrypted=bi0mpJJNxiYpEka5C6JO4nbsqxf4oCaRJp1ugbCveJoXHUsAQcB5wlhB4XV/qqCF
workflow.datasource.dialect=org.hibernate.dialect.MySQLInnoDBDialect
workflow.datasource.poolsize.max=32
workflow.datasource.poolsize.max.webapp=48
workflow.datasource.poolsize.max.appmaster=8
# ==================================================
# le-quartzclient/conf/env/prodcluster/quartzclient.properties
# ==================================================
quartzclient.datasource.driver=com.mysql.jdbc.Driver
quartzclient.datasource.url=jdbc:mysql://lpi-encrypted-cluster.cluster-c6q8lwiagbkt.us-east-1.rds.amazonaws.com/QuartzDB_b?autoReconnect=true&useSSL=false
quartzclient.datasource.user=LPI
quartzclient.datasource.password.encrypted=bi0mpJJNxiYpEka5C6JO4nbsqxf4oCaRJp1ugbCveJoXHUsAQcB5wlhB4XV/qqCF
quartzclient.datasource.dialect=org.hibernate.dialect.MySQLInnoDBDialect
quartzclient.datasource.type=MySQL
quartzclient.datasource.poolsize.max=4
quartzclient.datasource.poolsize.max.webapp=8
quartzclient.datasource.poolsize.max.appmaster=4
# ==================================================
# le-dantedb/conf/env/prodcluster/dantedb.properties
# ==================================================
dantedb.datasource.driver=com.microsoft.sqlserver.jdbc.SQLServerDriver
dantedb.datasource.url=jdbc:sqlserver://ddi06qnctteens.c6q8lwiagbkt.us-east-1.rds.amazonaws.com;databaseName=dante;
dantedb.datasource.user=lpiDanteUser
dantedb.datasource.password.encrypted=vg9jdCMngaZ+PqRrjgGhSw==
dantedb.datasource.dialect=org.hibernate.dialect.SQLServerDialect
dantedb.datasource.type=SQLServer
dantedb.datasource.dbname=dante
# ==================================================
# le-datadb/conf/env/prodcluster/db.properties
# ==================================================
datadb.datasource.driver=com.mysql.jdbc.Driver
datadb.datasource.url=jdbc:mysql://lpi-data-cluster.cluster-c6q8lwiagbkt.us-east-1.rds.amazonaws.com/Data_MultiTenant?autoReconnect=true&useSSL=false
datadb.datasource.sqoop.url=jdbc:mysql://lpi-data-cluster.cluster-c6q8lwiagbkt.us-east-1.rds.amazonaws.com/Data_MultiTenant?autoReconnect=true&useSSL=false
datadb.datasource.user=LPI
datadb.datasource.password.encrypted=bi0mpJJNxiYpEka5C6JO4nbsqxf4oCaRJp1ugbCveJoXHUsAQcB5wlhB4XV/qqCF
datadb.datasource.dialect=org.hibernate.dialect.MySQLInnoDBDialect
datadb.datasource.type=MySQL
datadb.datasource.poolsize.max=8
datadb.datasource.poolsize.max.webapp=16
datadb.datasource.poolsize.max.appmaster=8
# ==================================================
# le-dataplatform/conf/env/prodcluster/hadoop-metrics2.properties
# ==================================================
#
#   Licensed to the Apache Software Foundation (ASF) under one or more
#   contributor license agreements.  See the NOTICE file distributed with
#   this work for additional information regarding copyright ownership.
#   The ASF licenses this file to You under the Apache License, Version 2.0
#   (the "License"); you may not use this file except in compliance with
#   the License.  You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS,
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#   See the License for the specific language governing permissions and
#   limitations under the License.
#

# syntax: [prefix].[source|sink].[instance].[options]
# See javadoc of package-info.java for org.apache.hadoop.metrics2 for details

ledpjob.sink.ganglia.class=org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31
ledpjob.sink.ganglia.servers=bodcprodvhdp198.prod.lattice.local:8659
ledpjob.sink.ganglia.tagsForPrefix.ledpjob=AppId,ContainerId,Priority,Queue,Customer
ledpjob.sink.*.period=5

# ==================================================
# le-dataplatform/conf/env/prodcluster/dataplatform.properties
# ==================================================
dataplatform.yarn.job.basedir=/modeling-job
dataplatform.yarn.job.runtime.config=runtimeconfig.properties
dataplatform.customer.basedir=/user/s-analytics/customers
dataplatform.modeling.row.threshold=50
dataplatform.retry.wait.time=60000
dataplatform.sqoopjob.core.pool.size=10
dataplatform.sqoopjob.max.pool.size=90
dataplatform.sqoopjob.queue.capacity=90
dataplatform.completedjob.querylimit=1000
dataplatform.queue.scheme=dedicated
dataplatform.hdfs.stack=b
dataplatform.python.conda.env=lattice
# ------
dataplatform.dlorchestrationjob.core.pool.size=10
dataplatform.dlorchestrationjob.max.pool.size=10
dataplatform.dlorchestrationjob.queue.capacity=90
dataplatform.dlorchestrationjob.wait.time=180
dataplatform.dlorchestrationjob.row.fail.threshold=500
dataplatform.dlorchestrationjob.row.warn.threshold=1000
dataplatform.dlorchestrationjob.postiveevent.fail.threshold=100
dataplatform.dlorchestrationjob.postiveevent.warn.threshold=500
dataplatform.dlorchestration.datasource.dialect=org.hibernate.dialect.SQLServerDialect
dataplatform.dlorchestration.datasource.driver=com.microsoft.sqlserver.jdbc.SQLServerDriver
dataplatform.dlorchestration.datasource.url=jdbc:sqlserver://bodcprodvsql229.prod.lattice.local:1433;databaseName=LeadScoringDB;
dataplatform.dlorchestration.datasource.user=LeadScoringDBUser
dataplatform.dlorchestration.datasource.password.encrypted=fhXDHtk4A+l1UJsb6rfaqA==
dataplatform.dlorchestration.datasource.host=bodcprodvsql229.prod.lattice.local
dataplatform.dlorchestration.datasource.port=1433
dataplatform.dlorchestration.datasource.dbname=LeadScoringDB
dataplatform.dlorchestration.datasource.type=SQLServer
dataplatform.dlorchestration.metadataerror.fail=true
dataplatform.container.virtualcores=7
dataplatform.container.memory=56320
dataplatform.container.map.virtualcores=4
dataplatform.container.reduce.virtualcores=2
dataplatform.container.mapreduce.memory=7168
dataplatform.throttle.threshold=604800000
#test data source ----
dataplatform.test.datasource.user=root
dataplatform.test.datasource.password.encrypted=KPpl2JWz+k79LWvYIKz6cA==
dataplatform.test.datasource.host=BODCPRODVSQL123.prod.lattice.local
dataplatform.test.datasource.port=1433
dataplatform.test.datasource.dbname=LeadScoringDB
dataplatform.test.datasource.type=SQLServer
#parallel
dataplatform.model.parallel.enabled=false
dataplatform.sampling.parallel.trainingset.number=10
dataplatform.profiling.parallel.mapper.number=30
dataplatform.container.parallel.map.memory=14336
dataplatform.container.parallel.reduce.memory=28762

#mapreduce property
dataplatform.mapreduce.task.timeout=7200000

dataplatform.trustore.jks=


# ==================================================
# le-ldc-collectiondb/conf/env/prodcluster/ldc_collectiondb.properties
# ==================================================
ldc_collectiondb.datasource.driver=com.mysql.jdbc.Driver
ldc_collectiondb.datasource.url=jdbc:mysql://lpi-data-cluster.cluster-c6q8lwiagbkt.us-east-1.rds.amazonaws.com/LDC_CollectionDB?autoReconnect=true&useSSL=false
ldc_collectiondb.datasource.reader.url=jdbc:mysql://lpi-data-cluster.cluster-ro-c6q8lwiagbkt.us-east-1.rds.amazonaws.com/LDC_CollectionDB?autoReconnect=true&useSSL=false
ldc_collectiondb.datasource.user=LPI
ldc_collectiondb.datasource.password.encrypted=bi0mpJJNxiYpEka5C6JO4nbsqxf4oCaRJp1ugbCveJoXHUsAQcB5wlhB4XV/qqCF
ldc_collectiondb.datasource.dialect=org.hibernate.dialect.MySQLInnoDBDialect
ldc_collectiondb.datasource.type=MySQL
ldc_collectiondb.datasource.poolsize.max=8
ldc_collectiondb.datasource.poolsize.max.webapp=32
ldc_collectiondb.datasource.poolsize.max.appmaster=8
ldc_collectiondb.datasource.reader.poolsize.max=8
ldc_collectiondb.datasource.reader.poolsize.max.webapp=32
ldc_collectiondb.datasource.reader.poolsize.max.appmaster=8
# ==================================================
# le-dante/conf/env/prodcluster/dante.properties
# ==================================================
common.dante.url=https://pls.lattice-engines.com/index.aspx

# ==================================================
# le-microservice/core/conf/env/prodcluster/microservice.properties
# ==================================================
microservice.rest.endpoint.hostport=https://localhost:8080
microservice.modules=modeling,eai,metadata,scoring,workflowapi,dataflowapi,datacloudapi,modelquality,sqoop
microservice.apps=admin,microservice,scoringapi,matchapi

microservice.admin.health.url=https://localhost:8085/admin/health
microservice.pls.health.url=https://localhost:8081/pls/health
microservice.oauth2.health.url=https://localhost:8072/health
microservice.playmaker.health.url=https://localhost:8071/health
microservice.scoringapi.health.url=https://localhost:8073/score/health
microservice.matchapi.health.url=https://localhost:8076/match/health
microservice.microservice.health.url=https://localhost:8080/doc/health
microservice.ulysses.health.url=https://localhost:8075/ulysses/health
# ==================================================
# le-aws/conf/env/prodcluster/aws.properties
# ==================================================
aws.region=us-east-1

aws.default.access.key=AKIAJXONERX76R4HVBEQ
aws.default.secret.key.encrypted=bi0mpJJNxiYpEka5C6JO4k6DQFG6dKs3IRJglYQq5x72PnN/w4zi2cmTvgAqgol4QUZX0pFov2GRYTLkAbKycUtLWZJFjmfZRd78uDYw9Ns=
aws.customer.access.key=AKIAYR65XI26BQI4O44M
aws.customer.secret.key.encrypted=bi0mpJJNxiYpEka5C6JO4kcc2zjmPff2Ly1kfCNucAm0vF+11DfITk+aXfuqU9HWHMBfnh045tiWbj1L8/q+GS+46NBWxGI0ocNpGVskmNQ=

aws.s3.bucket=latticeengines-prodcluster
aws.customer.account.id=588338644668

aws.emr.cluster=
aws.dynamo.endpoint=
aws.elasticache.cluster.name=prod-encrypted

aws.test.s3.bucket=latticeengines-test-prodcluster
aws.test.emr.cluster=
aws.test.customer.access.key=
aws.test.customer.secret.key.encrypted=bi0mpJJNxiYpEka5C6JO4qm733fGidS8PyuKAZFqKIb9yb4hknHabZfpAJu526xebnC8cKbEbkFQvV159j9EBNbRpllctLmQ1nx9gir2Kak=
aws.customer.s3.bucket=latticeengines-prod-customers
aws.customer.iam.group=Customers

# ==================================================
# le-dellebi/conf/env/prodcluster/dellebi.properties
# ==================================================
#dellebi.datahadoopinpath is the directory to place the data which waits for importing to HDFS.
dellebi.camelunzipoutputpath=hdfs2://prodclusternameservice/latticeengines/in
dellebi.cascadinginpath=hdfs://prodclusternameservice/latticeengines/in
dellebi.datahadoopinpath=/latticeengines/in
dellebi.cascadinginputdelimiter=|~|

dellebi.datahadoopworkingpath=/user/dellebi
dellebi.datahadooperrorworkingpath=/user/dellebi/errordata
dellebi.datahadooprootpath=hdfs://prodclusternameservice
dellebi.datainputfiletype=/*.txt

dellebi.inputfileregex=tgt_(quote_trans_global).+(.zip)
dellebi.ordersummary=order_summary
dellebi.orderdetail=order_detail
dellebi.shiptoaddrlattice=ship_to_addr_lattice
dellebi.warrantyglobal=warranty_global
dellebi.quotetrans=quote_trans
dellebi.quotetrans.storeprocedure=SP_QUOTE_STAGE_UPDATE
dellebi.output.hdfsdata.remove=true

#Target DB Info
dellebi.datatarget.dialect=org.hibernate.dialect.SQLServerDialect
dellebi.datatarget.driver=com.microsoft.sqlserver.jdbc.SQLServerDriver
dellebi.datatarget.url=jdbc:sqlserver://10.51.15.145:1433;database=DELL_EBI_STAGE_QUOTES
dellebi.datatarget.host=10.51.15.145
dellebi.datatarget.port=1433
dellebi.datatarget.dbname=DELL_EBI_STAGE_QUOTES
dellebi.datatarget.type=SQLServer
dellebi.datatarget.user=hadoop
dellebi.datatarget.password.encrypted=8xuq8yYNOoNtHQpFel/J7w==
dellebi.datatarget.stagefinal.dbname=DELL_EBI_STAGE_FINAL_USE

dellebi.sqoopexporter.mapper.number=8

dellebi.configdb.datasource.dialect=org.hibernate.dialect.SQLServerDialect
dellebi.configdb.datasource.driver=com.microsoft.sqlserver.jdbc.SQLServerDriver
dellebi.configdb.datasource.url=jdbc:sqlserver://10.51.15.145:1433;database=DELL_EBI_CONTROL
dellebi.configdb.datasource.user=hadoop
dellebi.configdb.datasource.password.encrypted=8xuq8yYNOoNtHQpFel/J7w==
dellebi.configdb.datasource.host=10.51.15.145
dellebi.configdb.datasource.port=1433
dellebi.configdb.datasource.dbname=DELL_EBI_CONTROL
dellebi.configdb.datasource.type=SQLServer

dellebi.customer="Dell EBI"
dellebi.output.table.sample=PRE_STAGE_STG_QUOTE_DATA

dellebi.fileTypes.dellebiManagerJob1=quote
dellebi.fileTypes.dellebiManagerJob2=Order_Detail,SKU_Global,Channel,Warranty
dellebi.fileTypes.dellebiManagerJob3=Order_Summary,SKU_Itm_Cls_Code,Calendar,Account_Cust,SKU_Manufacturer
dellebi.dellebiManagerJob.schedule=0 0/5 * * * ?

dellebi.mailhost=10.51.1.60
dellebi.mailfrom=DBA@lattice-engines.com
dellebi.mailreceivelist=lming@lattice-engines.com,jwilliams@lattice-engines.com,Team.dell@lattice-engines.com,leibman@Lattice-Engines.com,dba@lattice-engines.com

dellebi.smbaccount=LATTICE\\s-hadoop
dellebi.smbps=#a5cHe@d00p!

dellebi.local.inboxpath=/data/dellebi
dellebi.local.datatarget.dbname=DELL_EBI_STAGE_QUOTES_HISTORY

dellebi.env=PRODUCTION

# ==================================================
# le-quartz/conf/env/prodcluster/quartz.properties
# ==================================================
quartz.microservice.rest.endpoint.hostport=
quartz.test.functional.api=
quartz.test.functional.testdesturl=http://www.w3school.com.cn/example/jquery/demo_test_post.asp

quartz.jobStore.driver.delegate=org.quartz.impl.jdbcjobstore.StdJDBCDelegate

quartz.scheduler.jobs.history.displaycount=5

quartz.scheduler.jobs.history.retaining.days=30

quartz.scheduler.jobs.history.cleanup.trigger=0 0 5 * * ?

#predefined jobs
quartz.predefined.jobs.enabled=

#pls
quartz.pls.predefined.job.primary.url=https://internal-public-lpi-b-556082394.us-east-1.elb.amazonaws.com/pls/quartzjob/triggerjob
quartz.pls.predefined.job.secondary.url=https://internal-public-lpi-b-556082394.us-east-1.elb.amazonaws.com/pls/quartzjob/triggerjob
quartz.pls.predefined.job.query.api=${HTTP_PROTOCOL}://%s/pls/quartzjob/checkactivejob
quartz.pls.predefined.job.check.bean.url=https://internal-public-lpi-b-556082394.us-east-1.elb.amazonaws.com/pls/quartzjob/checkjobbean
quartz.pls.modelsummarydownload.job.cron=0/15 * * * * ?
quartz.pls.segmentexportcleanup.job.cron=0 0 */6 * * ?
quartz.pls.modelsummarydownload.job.timeout=300
quartz.pls.segmentexportcleanup.job.timeout=300

#lp
quartz.lp.predefined.job.primary.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com/lp/quartzjob/triggerjob
quartz.lp.predefined.job.secondary.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com/lp/quartzjob/triggerjob
quartz.lp.predefined.job.query.api=${HTTP_PROTOCOL}://%s/lp/quartzjob/checkactivejob
quartz.lp.predefined.job.check.bean.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com/lp/quartzjob/checkjobbean
quartz.lp.modelsummarydownload.job.cron=0/15 * * * * ?
quartz.lp.modelsummarydownload.job.timeout=300

#playmaker
quartz.playmaker.predefined.job.primary.url=https://internal-public-lpi-b-556082394.us-east-1.elb.amazonaws.com/playmaker/quartzjob/triggerjob
quartz.playmaker.predefined.job.secondary.url=https://internal-public-lpi-b-556082394.us-east-1.elb.amazonaws.com/playmaker/quartzjob/triggerjob
quartz.playmaker.predefined.job.query.api=${HTTP_PROTOCOL}://%s/playmaker/quartzjob/checkactivejob
quartz.playmaker.predefined.job.check.bean.url=https://internal-public-lpi-b-556082394.us-east-1.elb.amazonaws.com/playmaker/quartzjob/checkjobbean
quartz.playmaker.recommendationcleanup.job.cron=0 */3 * * * ?
quartz.playmaker.recommendationcleanup.job.timeout=300

#dataplatform
quartz.dataplatform.predefined.job.primary.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com/modeling/quartzjob/triggerjob
quartz.dataplatform.predefined.job.secondary.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com/modeling/quartzjob/triggerjob
quartz.dataplatform.predefined.job.query.api=${HTTP_PROTOCOL}://%s/modeling/quartzjob/checkactivejob
quartz.dataplatform.predefined.job.check.bean.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com/modeling/quartzjob/checkjobbean
quartz.dataplatform.dlorchestration.job.cron=0/15 * * * * ?
quartz.dataplatform.jobwatchdog.job.cron=0/20 * * * * ?
quartz.dataplatform.dlorchestration.job.timeout=300
quartz.dataplatform.jobwatchdog.job.timeout=300

#dellebi
quartz.dellebi.predefined.job.primary.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com/dellebi/quartzjob/triggerjob
quartz.dellebi.predefined.job.secondary.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com/dellebi/quartzjob/triggerjob
quartz.dellebi.predefined.job.query.api=${HTTP_PROTOCOL}://%s/dellebi/quartzjob/checkactivejob
quartz.dellebi.predefined.job.check.bean.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com/dellebi/quartzjob/checkjobbean
quartz.dellebi.dailyjob1.job.cron=0 0/5 * * * ?
quartz.dellebi.dailyjob2.job.cron=20 0/5 * * * ?
quartz.dellebi.dailyjob3.job.cron=40 0/5 * * * ?
quartz.dellebi.dailyjob1.job.timeout=3600
quartz.dellebi.dailyjob2.job.timeout=3600
quartz.dellebi.dailyjob3.job.timeout=3600

#datacloudapi
quartz.datacloudapi.predefined.job.primary.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com/datacloudapi/quartzjob/triggerjob
quartz.datacloudapi.predefined.job.secondary.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com/datacloudapi/quartzjob/triggerjob
quartz.datacloudapi.predefined.job.query.api=${HTTP_PROTOCOL}://%s/datacloudapi/quartzjob/checkactivejob
quartz.datacloudapi.predefined.job.check.bean.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com/datacloudapi/quartzjob/checkjobbean
quartz.datacloudapi.refresh.job.cron=0 */10 * * * ?
quartz.datacloudapi.refresh.job.timeout=300
#datacloudcollection
#other properties will copy from datacloudapi.
quartz.datacloudcollection.job.cron=0/30 * * * * ?
quartz.datacloudcollection.job.timeout=3000

#scoring
quartz.scoring.predefined.job.primary.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com/scoring/quartzjob/triggerjob
quartz.scoring.predefined.job.secondary.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com/scoring/quartzjob/triggerjob
quartz.scoring.predefined.job.query.api=${HTTP_PROTOCOL}://%s/scoring/quartzjob/checkactivejob
quartz.scoring.predefined.job.check.bean.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com/scoring/quartzjob/checkjobbean
quartz.scoring.scoringmanager.job.cron=0/15 * * * * ?
quartz.scoring.scoringmanager.job.timeout=300

#matchapi.MatchCommandClean
quartz.matchapi.matchcommandclean.predefined.job.primary.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com/match/quartzjob/triggerjob
quartz.matchapi.matchcommandclean.predefined.job.secondary.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com/match/quartzjob/triggerjob
quartz.matchapi.matchcommandclean.predefined.job.query.api=${HTTP_PROTOCOL}://%s/match/quartzjob/checkactivejob
quartz.matchapi.matchcommandclean.predefined.job.check.bean.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com/match/quartzjob/checkjobbean
quartz.matchapi.matchcommandclean.refresh.job.cron=0 0 0 1/1 * ? *
quartz.matchapi.matchcommandclean.refresh.job.timeout=900

#cdl
quartz.cdl.predefined.job.primary.url=https://internal-public-lpi-b-556082394.us-east-1.elb.amazonaws.com/cdl/quartzjob/triggerjob
quartz.cdl.predefined.job.secondary.url=https://internal-public-lpi-b-556082394.us-east-1.elb.amazonaws.com/cdl/quartzjob/triggerjob
quartz.cdl.predefined.job.query.api=https://internal-public-lpi-b-556082394.us-east-1.elb.amazonaws.com/cdl/quartzjob/checkactivejob
quartz.cdl.predefined.job.check.bean.url=https://internal-public-lpi-b-556082394.us-east-1.elb.amazonaws.com/cdl/quartzjob/checkjobbean
quartz.cdl.import.job.cron=0 0 2 * * ?
quartz.cdl.orchestration.job.cron=0/30 * * * * ?
quartz.cdl.import.job.timeout=300
quartz.cdl.orchestration.job.timeout=300

#admin
quartz.admin.predefined.job.primary.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com/admin/quartzjob/triggerjob
quartz.admin.predefined.job.secondary.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com/admin/quartzjob/triggerjob
quartz.admin.predefined.job.query.api=${HTTP_PROTOCOL}://%s:9085/admin/quartzjob/checkactivejob
quartz.admin.predefined.job.check.bean.url=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com/admin/quartzjob/checkjobbean
quartz.admin.recycletenant.job.cron=0 0 12 * * ?
quartz.admin.recycletenant.job.timeout=3000

# ==================================================
# le-documentdb/conf/env/prodcluster/documentdb.properties
# ==================================================
documentdb.datasource.driver=com.mysql.jdbc.Driver
documentdb.datasource.url=jdbc:mysql://lpi-data-cluster.cluster-c6q8lwiagbkt.us-east-1.rds.amazonaws.com/DocumentDB?autoReconnect=true&useSSL=false
documentdb.datasource.reader.url=jdbc:mysql://lpi-data-cluster.cluster-ro-c6q8lwiagbkt.us-east-1.rds.amazonaws.com/DocumentDB?autoReconnect=true&useSSL=false
documentdb.datasource.user=LPI
documentdb.datasource.password.encrypted=bi0mpJJNxiYpEka5C6JO4nbsqxf4oCaRJp1ugbCveJoXHUsAQcB5wlhB4XV/qqCF
documentdb.datasource.dialect=org.hibernate.dialect.MySQLInnoDBDialect
documentdb.datasource.type=MySQL
documentdb.datasource.poolsize.max=8
documentdb.datasource.poolsize.max.webapp=16
documentdb.datasource.poolsize.max.appmaster=8
documentdb.datasource.reader.poolsize.max=8
documentdb.datasource.reader.poolsize.max.webapp=16
documentdb.datasource.reader.poolsize.max.appmaster=8
# ==================================================
# le-monitor/conf/env/prodcluster/monitor.properties
# ==================================================
monitor.alert.service.enabled=true

monitor.influxdb.enabled=true
monitor.influxdb.url=http://internal-influxdb-1572849568.us-east-1.elb.amazonaws.com:8086
monitor.influxdb.username=root
monitor.influxdb.password=HLUzT78mryzkqNtwtma0XA==
monitor.influxdb.environment=Production
monitor.influxdb.stack=b

monitor.emailsettings.from=no-reply@lattice-engines.com
monitor.emailsettings.server=smtprelay.prod.lattice.local
monitor.emailsettings.username=build@lattice-engines.com
monitor.emailsettings.password=ryTy2wEiaw51Le9qTXxx3w==
monitor.emailsettings.port=25
monitor.emailsettings.useSSL=false
monitor.emailsettings.useSTARTTLS=false
monitor.email.businessops=businessops@lattice-engines.com

monitor.email.pm=product@lattice-engines.com

monitor.health.inspection.enabled=true
# ==================================================
# le-graph/conf/env/prodcluster/graph.properties
# ==================================================
graph.ns.env=prodcluster
graph.ns.version=1.0
graph.ns.is.postfix=true
graph.exception.ignore=true
graph.contact.url=lpi-graph-cluster.cluster-c6q8lwiagbkt.us-east-1.neptune.amazonaws.com
graph.contact.port=8182


# ==================================================
# le-transform/conf/env/prodcluster/transform.properties
# ==================================================
transform.transformer.cache.maxsize=500
# ==================================================
# le-modelquality/conf/env/prodcluster/modelquality.properties
# ==================================================
modelquality.file.upload.hdfs.dir=/Pods/Default/Services/ModelQuality
modelquality.pls.login.tenant=ModelQuality_Test.ModelQuality_Test.Production
modelquality.pls.login.username=modelQuality@lattice-engines.com
modelquality.pls.login.password=Lattice123
modelquality.pls.login.password.hash=3OCRIbECCiTtJ8FyaNgvTjNES/eyjQUK59Z5rMCnrAk=
# ==================================================
# le-datacloudapi/conf/env/prodcluster/propdata.properties
# ==================================================
#Collection
propdata.collection.sqoop.mapper.number=16
propdata.collection.cascading.partitions=32

propdata.job.default.schedule=*/10 * * * * ?
propdata.job.legacy.heartbeat.schedule=0 * * * * ?
propdata.job.engine.heartbeat.schedule=0 * * * * ?
propdata.job.schedule.dryrun=false

# BuiltWith - Saturday, 3 AM (East)
propdata.job.buitwith.archive.schedule=0 0 1 ? * SAT
propdata.job.buitwith.refresh.schedule=0 0 3 ? * SAT
propdata.job.buitwith.pivot.schedule=0 0 6 ? * SAT

# Alexa - Saturday, 4 PM (East)
propdata.job.alexa.archive.schedule=0 0 16 ? * SAT
propdata.job.alexa.refresh.schedule=0 0 18 ? * SAT

# Orb - Tuesday, 3 AM (East)
propdata.job.orb.archive.schedule=0 0 1 ? * TUE
propdata.job.orb.refresh.schedule=0 0 3 ? * TUE

# Feature - Wednesday, 3 AM (East)
propdata.job.feature.archive.schedule=0 0 1 ? * WED
propdata.job.feature.refresh.schedule=0 0 3 ? * WED
propdata.job.feature.pivot.schedule=0 0 6 ? * WED

# HGData - 18th every month, 4 AM (East)
propdata.job.hgdata.archive.schedule=0 0 4 18 * ?
propdata.job.hgdata.refresh.schedule=0 0 6 18 * ?
propdata.job.hgdata.pivot.schedule=0 0 9 18 *  ?

# related to Bombora source
# bomboraFirehose - every two hour
propdata.job.bomboraFirehose.archive.schedule=0 0 */2 * * *
# bomboraDepivoted - every day, 3 PM (East)
propdata.job.bomboradepivoted.fixedinterval.schedule=0 0 15 * * *

# test
propdata.test.env=ProdCluster
propdata.api.functional.hostport=
propdata.test.match.client=PD130

# ==================================================
# le-cache/conf/env/prodcluster/cache.properties
# ==================================================
cache.type=redis
cache.redis.command.timeout.min=1
# ==================================================
# le-api/conf/env/prodcluster/api.properties
# ==================================================
api.rest.endpoint.hostport=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com
documentation.services.version=1.0
documentation.services.basePath=https://internal-private-lpi-b-1755219837.us-east-1.elb.amazonaws.com/rest
# ==================================================
# le-admin/conf/env/prodcluster/admin.properties
# ==================================================
admin.bardjams.dryrun=true
admin.dante.dryrun=true
admin.vdbdl.dryrun=true
admin.pls.dryrun=false
admin.cdl.dryrun=false
admin.vdb.tpl.dryrun=true
admin.dl.tpl.dryrun=true
admin.eai.dryrun=false
admin.metadata.dryrun=false
admin.modeling.dryrun=false

admin.upload.schema=true

# the jetty server suppose only reads/writes system files under this directory
admin.mount.rootpath=/mnt
admin.mount.vdb.permstore=visidb
admin.mount.dl.datastore=dl
admin.mount.tpl=tpl

#Dante
admin.dante.hosts=https://bis.lattice-engines.com/BuyerInsights

#Bard JAMS
admin.bardjams.datasource.dialect=org.hibernate.dialect.SQLServerDialect
admin.bardjams.datasource.driver=com.microsoft.sqlserver.jdbc.SQLServerDriver
admin.bardjams.datasource.url=jdbc:sqlserver://bodcprodvsql200.prod.lattice.local\\SQL200;databaseName=JAMSCFG;
admin.bardjams.datasource.user=s-tenantconsole
admin.bardjams.datasource.password.encrypted=S4WzJNhiYqvTW/yw/+GYOQ==

#Modeling
admin.modelingservice.basedir=/user/s-analytics/customers/

# overwrite default configuration and schema
admin.overwrite.pls.superadmin=rswarts@lattice-engines.com,syin@lattice-engines.com,thummel@lattice-engines.com,jxia@Lattice-Engines.com
admin.overwrite.pls.latticeadmin=marketoconnectoruser@lattice-engines.com
admin.overwrite.dl.url=https://data-pls.lattice-engines.com/Dataloader_PLS
admin.overwrite.dl.url.options=https://data-pls.lattice-engines.com/Dataloader_PLS,https://dataloader-prod.lattice-engines.com/dataloader_pls
admin.overwrite.dl.owner=admin.dataloader@lattice-engines.com
admin.overwrite.dl.datastore=\\\\10.51.1.150\\datastore\\DataLoader\\Customers
admin.overwrite.vdb.permstore=\\\\Bodcprodvfil113\\prodstorage2
admin.overwrite.vdb.permstore.options=\\\\Bodcprodvfil113\\prodstorage2,\\\\prodstorage\\visidb\\PermStore
admin.overwrite.vdb.servername=BODCPRODVVDB190
admin.overwrite.vdb.servername.options=BODCPRODVVDB190,BODCPRODVVDB153,BODCPRODVVDB160,BODCPRODVVDB161,BODCPRODVVDB164,BODCPRODVVDB165,BODCPRODVVDB168,BODCPRODVVDB173,BODCPRODVVDB174,BODCPRODVVDB177,BODCPRODVVDB180,BODCPRODVVDB181,BODCPRODVVDB184,BODCPRODVVDB191,BODCPRODVVDB192,BODCPRODVVDB208

# properties for test
admin.test.contract=
admin.test.dl.url=
admin.test.vdb.servername=
admin.test.vdb.permstore.server=
admin.test.dl.user=
admin.test.dl.datastore.server=
admin.test.dl.datastore.path=

admin.test.functional.api=

# ==================================================
# le-dataflowapi/conf/env/prodcluster/dataflowapi.properties
# ==================================================
dataflowapi.engine=TEZ
dataflowapi.checkpoint=false
dataflowapi.am.mem=8196
dataflowapi.flink.local.vcores=2
dataflowapi.flink.local.mem=16384
# ==================================================
# le-yarn/conf/env/prodcluster/yarn.properties
# ==================================================
yarn.jacoco.enabled=false
yarn.jacoco.data.dir=/tmp/jacoco

# url used in yarn
yarn.pls.url=https://internal-public-lpi-b-556082394.us-east-1.elb.amazonaws.com

# functional test
yarn.use.minicluster=false
# ==================================================
# le-ulysses/conf/env/prodcluster/ulysses.properties
# ==================================================

# ==================================================
# le-proxy/conf/env/prodcluster/proxy.properties
# ==================================================
proxy.retry.initialwaitmsec=500
proxy.retry.multiplier=2
proxy.retry.maxattempts=10

proxy.test.rest.endpoint.hostport=
proxy.test.keystore.path=
# ==================================================
# le-datafabric/conf/env/prodcluster/datafabric.properties
# ==================================================
datafabric.disabled=false
datafabric.message.brokers=10.51.1.78:9092,10.51.1.79:9092,10.51.1.114:9092
datafabric.message.zkConnect=10.51.1.78:2181,10.51.1.79:2181,10.51.1.114:2181
datafabric.message.environment=prodcluster
datafabric.message.stack=global
datafabric.message.schemaRegUrl=http://10.51.1.114:8081
datafabric.message.version=1.0.0
datafabric.dataService.redis.servers=10.51.1.115,10.51.1.116,10.51.1.117
datafabric.dataService.redis.port=26379
datafabric.dataService.redis.ha.enabled=true
datafabric.generic.entity.kafka.replication=2

# ==================================================
# le-camille/conf/env/prodcluster/camille.properties
# ==================================================
camille.zk.connectionString=zklayer4.prod.lattice.local,zklayer5.prod.lattice.local,zklayer6.prod.lattice.local
camille.zk.pod.id=Production
camille.zk.sharedQueues=bootstrap_Dante
camille.zk.division=b

# ==================================================
# le-scheduler/conf/env/prodcluster/capacity-scheduler.properties
# ==================================================
# Note this file is really more for informational purpose.  To actually affect the cluster, these properties must be entered into the Capacity Scheduler text box in the Ambari configuration for Yarn Scheduler.  Ambari will autogenerate/overwrite /etc/hadoop/conf/capacity-scheduler.xml with these properties.
# yarn.scheduler.capacity.root.<queuename>.maximum-capacity // !!! Note that the maximum-capacity is the percentage of non-reserved nodes.  Whereas the capacity is the percentage of all nodes.  Modeling is special case where maxCapacity is reserved+unreserved nodes.
# yarn.scheduler.capacity.root.Modeling.default-node-label-expression=reserved // set this to ensure all Modeling jobs go to the reserved nodes only.
# yarn.scheduler.capacity.root.Modeling.maximum-am-resource-percent // set this to get max number of schedulable modeling apps to 4 (current # of reserved modeling nodes)
# total unreserved % of cluster == 80%.
# modeling unreserved % of cluster == 12.5%.
# propdata unreserved % of cluster == 75%.
# scoring unreserved % of cluster == 30%. // allow flexing into modeling queue.  worst case 12.5%

yarn.scheduler.capacity.root.accessible-node-labels.reserved.capacity=100
yarn.scheduler.capacity.root.acl_administer_queue=*
yarn.scheduler.capacity.root.capacity=100
yarn.scheduler.capacity.root.Modeling.accessible-node-labels=reserved
yarn.scheduler.capacity.root.Modeling.accessible-node-labels.reserved.capacity=100
yarn.scheduler.capacity.root.Modeling.capacity=30
yarn.scheduler.capacity.root.Modeling.default-node-label-expression=reserved
yarn.scheduler.capacity.root.Modeling.maximum-am-resource-percent=0.04
yarn.scheduler.capacity.root.Modeling.maximum-capacity=30
yarn.scheduler.capacity.root.Modeling.state=RUNNING
yarn.scheduler.capacity.root.Modeling.user-limit-factor=10
yarn.scheduler.capacity.root.PropData.capacity=50
yarn.scheduler.capacity.root.PropData.maximum-am-resource-percent=0.02
yarn.scheduler.capacity.root.PropData.maximum-capacity=75
yarn.scheduler.capacity.root.PropData.state=RUNNING
yarn.scheduler.capacity.root.PropData.user-limit-factor=3
yarn.scheduler.capacity.root.queues=Scoring,Modeling,PropData
yarn.scheduler.capacity.root.Scoring.capacity=20
yarn.scheduler.capacity.root.Scoring.maximum-capacity=30
yarn.scheduler.capacity.root.Scoring.state=RUNNING
yarn.scheduler.capacity.root.Scoring.user-limit-factor=10

# ==================================================
# le-encryption/conf/env/prodcluster/encryption.properties
# ==================================================
encryption.enabled=false
encryption.key.policy=contract
encryption.key.policy.master.key.name=master

# ==================================================
# le-workflowapi/conf/env/prodcluster/workflowapi.properties
# ==================================================
workflowapi.modelingservice.basedir=/user/s-analytics/customers/

workflowapi.test.sfdc.user.name=rgonzalez2@lattice-engines.com
workflowapi.test.sfdc.passwd.encrypted=LVwTbIbeIMUI0A0TgsjksA==
workflowapi.test.sfdc.securitytoken=JYJz57SnYag3YWQ5caQxIP04b
workflowapi.test.accountmaster.path=/Pods/Production/Services/PropData/MatchService/AccountMaster

# ==================================================
# le-matchapi/conf/env/prodcluster/matchapi.properties
# ==================================================
matchapi.test.functional.hostport=
datacloud.core.accountmasterstats.numericbuckets.enabled=true
datacloud.core.accountmasterstats.dummybuckets.enabled=false

# ==================================================
# le-scoring/conf/env/prodcluster/scoring.properties
# ==================================================
scoring.core.pool.size=6
scoring.max.pool.size=6
scoring.queue.capacity=100
scoring.dao.datasource.dialect=org.hibernate.dialect.SQLServerDialect
scoring.dao.datasource.driver=com.microsoft.sqlserver.jdbc.SQLServerDriver
scoring.dao.datasource.user=s-scoring
scoring.dao.datasource.password.encrypted=miW8pkyili3mlxd4boI2TA==
scoring.dao.datasource.url=jdbc:sqlserver://BODCPRODVSQL200.prod.lattice.local\\SQL200;databaseName=Scoring_Daemon_PLS_2;user=$$USER$$;password=$$PASSWD$$
scoring.dao.datasource.type=SQLServer

scoring.output.table.sample=ScoreOutput
scoring.cleanup.timeinternval=120
scoring.test.table=2CheckoutTest
scoring.cleanup.enableCleanHdfs=false

scoring.mapper.threshold=10000
scoring.mapper.logdir=/var/log/scoring/mapper
scoring.mapper.max.input.split.size=10485760
scoring.mapper.min.input.split.size=10485760

scoring.processor.threadpool.size=14
scoring.processor.threadpool.timeoutmin=1440
scoring.processor.bulkrecord.size=200
scoring.processor.vcores=2
scoring.processor.memory=4096
# ==================================================
# le-serviceapps/cdl/conf/env/prodcluster/cdl.properties
# ==================================================
cdl.dataloader.tenant.mapping.enabled=true
cdl.transform.workflow.mem.mb=16384
cdl.modeling.workflow.mem.mb=8192
cdl.aps.generate.enabled=true
cdl.aps.generate.in.aws=true
cdl.play.service.threadpool.size=5
cdl.rating.service.threadpool.size=5
cdl.pa.default.max.iteration=2
cdl.rating.crossell.minimum.events=50
cdl.play.service.default.types=List,Cross-Sell,Prospecting,Renewal,Upsell
cdl.play.service.default.types.user=build-admin@lattice-engines.com
# ==================================================
# le-serviceapps/lp/conf/env/prodcluster/lp.properties
# ==================================================

# ==================================================
# le-auth/conf/env/prodcluster/auth.properties
# ==================================================
auth.globalauth.datasource.driver=com.mysql.jdbc.Driver
auth.globalauth.datasource.url=jdbc:mysql://lpi-encrypted-cluster.cluster-c6q8lwiagbkt.us-east-1.rds.amazonaws.com/GlobalAuthentication?autoReconnect=true&useSSL=false
auth.globalauth.datasource.user=LPI
auth.globalauth.datasource.password.encrypted=bi0mpJJNxiYpEka5C6JO4nbsqxf4oCaRJp1ugbCveJoXHUsAQcB5wlhB4XV/qqCF
auth.globalauth.hibernate.dialect=org.hibernate.dialect.MySQLInnoDBDialect
auth.globalauth.datasource.poolsize.max=8
auth.globalauth.datasource.poolsize.max.webapp=16
auth.globalauth.datasource.poolsize.max.appmaster=8

# ==================================================
# le-dataflow/conf/env/prodcluster/dataflow.properties
# ==================================================

# ==================================================
# le-hadoop/conf/env/prodcluster/hadoop.properties
# ==================================================
hadoop.use.emr=
hadoop.dfs.nameservices=ledpprod2
hadoop.dfs.client.failover.proxy.provider=org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider
dfs.namenode.rpc-address.nn1=bodcprodvhdp50.prod.lattice.local:8020
dfs.namenode.rpc-address.nn2=bodcprodvhdp51.prod.lattice.local:8020
hadoop.fs.defaultFS=hdfs://ledpprod2
hadoop.mapred.job.tracker=bodcprodvhdp51.prod.lattice.local:8050
# ------
hadoop.yarn.resourcemanager.ha.enabled=false
hadoop.yarn.resourcemanager.ha.rm-ids=rm1,rm2
hadoop.yarn.resourcemanager.ha.id=rm1
hadoop.yarn.resourcemanager.address.rm1=bodcprodvhdp51.prod.lattice.local:8050
hadoop.yarn.resourcemanager.scheduler.address.rm1=bodcprodvhdp51.prod.lattice.local:8030
hadoop.yarn.resourcemanager.webapp.address.rm1=bodcprodvhdp51.prod.lattice.local:8088
hadoop.yarn.resourcemanager.address.rm2=bodcprodvhdp51.prod.lattice.local:8050
hadoop.yarn.resourcemanager.scheduler.address.rm2=bodcprodvhdp51.prod.lattice.local:8030
hadoop.yarn.resourcemanager.webapp.address.rm2=bodcprodvhdp51.prod.lattice.local:8088
hadoop.yarn.client.failover-sleep-base-ms=10000
hadoop.yarn.client.failover-sleep-max-ms=300000
# ------
hadoop.yarn.resourcemanager.address=bodcprodvhdp51.prod.lattice.local:8050
hadoop.yarn.resourcemanager.scheduler.address=bodcprodvhdp51.prod.lattice.local:8030
hadoop.yarn.resourcemanager.webapp.address=bodcprodvhdp51.prod.lattice.local:8088
hadoop.mapreduce.jobhistory.webapp.api.address=http://bodcprodvhdp51.prod.lattice.local:19888/ws/v1/history/mapreduce/jobs
hadoop.mapreduce.jobhistory.address=bodcprodvhdp51.prod.lattice.local:10020
hadoop.mapreduce.jobhistory.webapp.address=bodcprodvhdp51.prod.lattice.local:19888
hadoop.yarn.timeline-service.webapp.address=http://bodcprodvhdp51.prod.lattice.local:8188/applicationhistory
hadoop.fs.web.defaultFS=http://webhdfs.prod.lattice.local:14000/webhdfs/v1
hadoop.fs.web.webhdfs=webhdfs://webhdfs.prod.lattice.local:14000
hadoop.yarn.nodemanager.remote-app-log-dir=/app-logs
hadoop.hadoop.security.key.provider.path=kms://http@bodcprodvhdp50.prod.lattice.local:9292/kms
hadoop.dfs.encryption.key.provider.uri=kms://http@bodcprodvhdp50.prod.lattice.local:9292/kms
hadoop.dfs.replace-datanode-on-failure.policy=DEFAULT
# -----
tez.lib.uris=/apps/tez/tez-0.9.0.tar.gz
# -----
hadoop.use.ambari=true
hadoop.ambari.yarn.cp=$HADOOP_CONF_DIR,/usr/hdp/current/hadoop-client/*,/usr/hdp/current/hadoop-client/lib/*,/usr/hdp/current/hadoop-hdfs-client/*,/usr/hdp/current/hadoop-hdfs-client/lib/*,/usr/hdp/current/hadoop-yarn-client/*,/usr/hdp/current/hadoop-yarn-client/lib/*,/usr/hdp/current/hadoop-mapreduce-client/lib/*,/usr/hdp/current/hadoop-mapreduce-client/*,/usr/hdp/current/tez-client/conf
hadoop.ambari.mr.cp=$HADOOP_CONF_DIR,/usr/hdp/current/hadoop-client/*,/usr/hdp/current/hadoop-client/lib/*,/usr/hdp/current/hadoop-hdfs-client/*,/usr/hdp/current/hadoop-hdfs-client/lib/*,/usr/hdp/current/hadoop-yarn-client/*,/usr/hdp/current/hadoop-yarn-client/lib/*,/usr/hdp/current/hadoop-mapreduce-client/lib/*,/usr/hdp/current/hadoop-mapreduce-client/*,/usr/hdp/current/tez-client/conf,$PWD/mr-framework/hadoop/share/hadoop/mapreduce/*:$PWD/mr-framework/hadoop/share/hadoop/mapreduce/lib/*:$PWD/mr-framework/hadoop/share/hadoop/common/*:$PWD/mr-framework/hadoop/share/hadoop/common/lib/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/*:$PWD/mr-framework/hadoop/share/hadoop/yarn/lib/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/*:$PWD/mr-framework/hadoop/share/hadoop/hdfs/lib/*:$PWD/mr-framework/hadoop/share/hadoop/tools/lib/*

# ==================================================
# le-security/conf/env/prodcluster/security.properties
# ==================================================
security.ldap.url=ldap://bodcprodvdc12.lattice.local
security.ldap.domain=lattice.local
security.app.public.url=https://app.lattice-engines.com
security.globalauth.url=https://globalauth.prod.lattice.local/Global_AuthenticationHA

security.test.api.hostport=

security.zendesk.enabled=false
security.zendesk.url=https://lattice-engines.zendesk.com/
security.zendesk.email=johndoe@lattice-engines.com
security.zendesk.encryptedApiToken=bi0mpJJNxiYpEka5C6JO4tTD2CJkHRaABQA/VYwQNaW8mBTxRp+0YQN5WBwl4ZRd


security.zendesk.jwt.redirecturl=https://lattice-engines.zendesk.com/access/jwt
security.zendesk.jwt.secretkey=zbcHrhuhVwTBwwY0ZTICn1aIpTXQNEe1L5gWwIFsAwEKTgwA
# ==================================================
# le-db/conf/env/prodcluster/db.properties
# ==================================================
db.datasource.driver=com.mysql.jdbc.Driver
db.datasource.url=jdbc:mysql://lpi-encrypted-cluster.cluster-c6q8lwiagbkt.us-east-1.rds.amazonaws.com/PLS_MultiTenant?autoReconnect=true&useSSL=false
db.datasource.reader.driver=com.mysql.jdbc.Driver
db.datasource.reader.url=jdbc:mysql://lpi-encrypted-cluster.cluster-ro-c6q8lwiagbkt.us-east-1.rds.amazonaws.com/PLS_MultiTenant?autoReconnect=true&useSSL=false
db.datasource.user=LPI
db.datasource.password.encrypted=bi0mpJJNxiYpEka5C6JO4nbsqxf4oCaRJp1ugbCveJoXHUsAQcB5wlhB4XV/qqCF
db.datasource.dialect=org.hibernate.dialect.MySQLInnoDBDialect
db.datasource.type=MySQL
db.datasource.show_sql=false
db.datasource.poolsize.max=32
db.datasource.poolsize.max.webapp=48
db.datasource.poolsize.max.appmaster=16
db.datasource.reader.poolsize.max=32
db.datasource.reader.poolsize.max.webapp=32
db.datasource.reader.poolsize.max.appmaster=16

# ==================================================
# le-redshiftdb/conf/env/prodcluster/redshift.properties
# ==================================================
redshift.root.user=batch
redshift.segment.user=segment
redshift.password.encrypted=bi0mpJJNxiYpEka5C6JO4kiZsWNIJj58/jmop/r+F0qBVI9wBbAE5qJJuRvrquQ7
redshift.jdbc.url=jdbc:redshift://lpi.ccimt3gfgqxe.us-east-1.redshift.amazonaws.com:5439
redshift.app.database=app
redshift.test.load.tests.per.thread=1


# ==================================================
# le-oauth2db/conf/env/prodcluster/oauth2.properties
# ==================================================
oauth2.datasource.dialect=org.hibernate.dialect.MySQLInnoDBDialect
oauth2.datasource.driver=com.mysql.jdbc.Driver
oauth2.datasource.url=jdbc:mysql://lpi-encrypted-cluster.cluster-c6q8lwiagbkt.us-east-1.rds.amazonaws.com/oauth2DB?autoReconnect=true&useSSL=false
oauth2.datasource.type=MySQL
oauth2.datasource.user=LPI
oauth2.datasource.password.encrypted=bi0mpJJNxiYpEka5C6JO4nbsqxf4oCaRJp1ugbCveJoXHUsAQcB5wlhB4XV/qqCF
oauth2.datasource.poolsize.max=64
oauth2.datasource.poolsize.max.webapp=64
oauth2.datasource.poolsize.max.appmaster=16

oauth2.password_expiration_days=1
# ==================================================
# le-saml/conf/env/prodcluster/saml.properties
# ==================================================
saml.lb.scheme=https
saml.lb.servername=app.lattice-engines.com
saml.lb.includeserverport=false
saml.lb.serverport=3000
saml.lb.contextpath=/pls/saml/login/

saml.metadata.refresh.frequency=5000
saml.timeout.authNInstant.hours=12

saml.keystore.resource.path.encrypted=bi0mpJJNxiYpEka5C6JO4vlPNZjHN1jIDsnR1ZIk7qxl1nz/pjl4rGPUW0NZtZ9D7dTiqsIFhJQdhj+pxQZ+uYkHtJUV+CwqC6Z/V1co/2M=
saml.keystore.defaultKey.encrypted=bi0mpJJNxiYpEka5C6JO4sBIJb3KRwhYHLIdd5fUHTrpEl7LO9Un8VkojEn4BW8yxIRJ5jNzAFQuUrMJTVfLOQ==
saml.keystore.password.encrypted=bi0mpJJNxiYpEka5C6JO4vqffppMiq58CaHeqMFI4SvgfF2ACdGs39AggGjR0yPV
saml.keystore.storePass.encrypted=bi0mpJJNxiYpEka5C6JO4vqffppMiq58CaHeqMFI4SvgfF2ACdGs39AggGjR0yPV

# ==================================================
# le-eai/conf/env/prodcluster/eai.properties
# ==================================================
eai.salesforce.production.loginurl=https://login.salesforce.com
eai.salesforce.sandbox.loginurl=https://test.salesforce.com
eai.salesforce.clientid=3MVG9CVKiXR7Ri5qgebamGAhq9fChr3m1hoj51ftmqQpl5IbCFpK.BOIOgR5uRH7YKz0epGD1yBpgvtRKUGPX
eai.salesforce.clientsecret=473823972728429785

eai.vdb.file.size=100000000

eai.max.redeliveries=5
eai.backoff.multiplier=2

eai.test.salesforce.username=apeters-widgettech@lattice-engines.com
eai.test.salesforce.password.encrypted=bi0mpJJNxiYpEka5C6JO4l3HqtRSPPdF/FPQG/J1yDqXLtkIj9S4rDIb7sZ5VaUA
eai.test.salesforce.securitytoken=oIogZVEFGbL3n0qiAp6F66TC
eai.test.metadata.port=8080
eai.test.metadata.url=
eai.test.sftp.host=10.41.1.31
eai.test.sftp.user=sftpdev
eai.test.sftp.password.encrypted=KPpl2JWz+k79LWvYIKz6cA==

eai.export.dynamo.num.mappers=32
eai.export.dynamo.signature=20180515

# ==================================================
# le-pls/conf/env/prodcluster/pls.properties
# ==================================================
pls.accountmaster.datasource.driver=com.microsoft.sqlserver.jdbc.SQLServerDriver
pls.accountmaster.datasource.url=jdbc:sqlserver://bodcprodvsql231.prod.lattice.local:1433;databaseName=LDC_SourceDB;
pls.accountmaster.datasource.user=DLTransfer
pls.accountmaster.datasource.password.encrypted=Q1nh4HIYGkg4OnQIEbEuiw==
pls.accountmaster.datasource.host=bodcprodvsql231.prod.lattice.local
pls.accountmaster.datasource.port=1433
pls.accountmaster.datasource.dbname=LDC_SourceDB
pls.accountmaster.datasource.type=SQLServer

pls.modelingservice.basedir=/user/s-analytics/customers/
pls.modeldeletion.core.pool.size=10
pls.modeldeletion.max.pool.size=20
pls.modeldeletion.queue.capacity=20

pls.downloader.core.pool.size=10
pls.downloader.max.pool.size=20
pls.downloader.queue.capacity=20
pls.jmx.downloader.check.frequency=600

pls.modelingservice.testdsdb=LeadScoringDB
pls.modelingservice.testdsdbhost=BODCPRODVSQL123.prod.lattice.local
pls.modelingservice.testdsdbuser=root
pls.modelingservice.testdsdbpasswd.encrypted=KPpl2JWz+k79LWvYIKz6cA==
pls.modelingservice.testdsdbport=1433

pls.dataloader.rest.api=https://data-pls.lattice-engines.com/Dataloader_PLS/DLRestService
pls.dataloader.sfdc.login.url=https://login.salesforce.com/services/Soap/u/33.0
pls.dataloader.marketo.login.url=https://na-sj02.marketo.com/soap/mktows/2_0
pls.dataloader.eloqua.login.url=https://login.eloqua.com/id

pls.modelalerts.min.success.events=500
pls.modelalerts.min.conversion.percentage=1
pls.modelalerts.min.rocscore=0.7
pls.modelalerts.max.rocscore=0.85
pls.modelalerts.max.discrete.values=200
pls.modelalerts.max.feature.importance=0.1
pls.modelalerts.max.null.lift=1

pls.default.buyerinsights.num.predictors=50

pls.ff.plstestflag=true
pls.ff.setuppage=true
pls.ff.adminalertstab=false
pls.ff.deploymentwizardpage=false
pls.ff.leadenrichmentpage=true

pls.current.stack=b

# for tests
pls.test.functional.api=
pls.test.deployment.reset.by.admin=false
pls.test.contract=ProductionTest
pls.test.tenant.reg.json=tenant-registration-prod.json
pls.test.sfdc.user.name=rgonzalez2@lattice-engines.com
pls.test.sfdc.passwd.encrypted=LVwTbIbeIMUI0A0TgsjksA==
pls.test.sfdc.securitytoken=JYJz57SnYag3YWQ5caQxIP04b
pls.test.security.api=


# workflow
pls.fitflow.stoplist.path=/Pods/Production/Services/PropData/MatchService/PublicDomain/*.avro
pls.accountmaster.path=/Pods/Production/Services/PropData/MatchService/AccountMaster

pls.cdl.transform.cascading.partitions=36
pls.cdl.transform.tez.task.mem.gb=4
pls.cdl.transform.sort.max.split.threads=8
pls.cdl.transform.default.cascading.engine=tez

pls.sureshot.map.creds.auth=https://score.lattice-engines.com/external/lattice/$$CRM_TYPE$$/credentials
pls.sureshot.scoring.settings=https://score.lattice-engines.com/external/lattice/score/$$CRM_TYPE$$/models
pls.sureshot.enrichment.settings=https://score.lattice-engines.com/external/lattice/score/$$CRM_TYPE$$/enrichment
pls.sureshot.playmaker.endpoint=https://api.lattice-engines.com
pls.fileupload.maxupload.bytes=500000000
pls.fileupload.maxupload.rows=1000000
pls.modeling.validation.min.rows=300
pls.modeling.validation.min.eventrows=50
pls..modeling.validation.min.negativerows=250

pls.sourcefile.retain.days=7

pls.scoring.use.rtsapi=false

pls.leadenrichment.premium.max=10
pls.marketo.enrichment.webhook.url=https://api.lattice-engines.com/score/enrich/record/
pls.marketo.scoring.webhook.resource=https://api.lattice-engines.com/score/external/record/

pls.rating.coverageservice.threshold.parallel=4
pls.rating.coverageservice.threadpool.size=25
pls.play.service.threadpool.size=25

pls.segment.export.max=1000000

pls.saml.local.redirection.allowed=false
# ==================================================
# le-playmaker/conf/env/prodcluster/playmaker.properties
# ==================================================
playmaker.datasource.dialect=org.hibernate.dialect.MySQLInnoDBDialect
playmaker.datasource.driver=com.mysql.jdbc.Driver
playmaker.datasource.url=jdbc:mysql://lpi-encrypted-cluster.cluster-c6q8lwiagbkt.us-east-1.rds.amazonaws.com/oauth2DB?autoReconnect=true&useSSL=false
playmaker.datasource.type=MySQL
playmaker.datasource.user=LPI
playmaker.datasource.password.encrypted=bi0mpJJNxiYpEka5C6JO4nbsqxf4oCaRJp1ugbCveJoXHUsAQcB5wlhB4XV/qqCF
playmaker.datasource.mssql.user=playmakerAPI
playmaker.datasource.mssql.password.encrypted=bi0mpJJNxiYpEka5C6JO4hm0DzOgT3hrOcIS08NB+jE7Ftfhs35h13BSxlZaX4IG

playmaker.jdbc.pool.min.size=0
playmaker.jdbc.pool.max.size=20
playmaker.jdbc.pool.max.idle=120
playmaker.jdbc.pool.max.checkout=60000

playmaker.update.bulk.max=1000
playmaker.recommendations.years.keep=2

# ==================================================
# le-datacloud/conf/env/prodcluster/datacloud.properties
# ==================================================
# match
datacloud.match.matcher.default.client=PD126
datacloud.match.matcher.available.clients=PD126,PD131,PD144

datacloud.manage.url=jdbc:mysql://lpi-encrypted-cluster.cluster-c6q8lwiagbkt.us-east-1.rds.amazonaws.com/LDC_ManageDB?autoReconnect=true&useSSL=false
datacloud.manage.reader.url=jdbc:mysql://lpi-encrypted-cluster.cluster-ro-c6q8lwiagbkt.us-east-1.rds.amazonaws.com/LDC_ManageDB?autoReconnect=true&useSSL=false
datacloud.manage.driver=com.mysql.jdbc.Driver
datacloud.manage.dialect=org.hibernate.dialect.MySQLInnoDBDialect
datacloud.manage.user=LPI
datacloud.manage.password.encrypted=bi0mpJJNxiYpEka5C6JO4nbsqxf4oCaRJp1ugbCveJoXHUsAQcB5wlhB4XV/qqCF
datacloud.manage.poolsize.max=16
datacloud.manage.poolsize.max.webapp=16
datacloud.manage.poolsize.max.appmaster=8

datacloud.match.block.interval.sec=5
datacloud.match.average.block.size=10000
datacloud.match.max.num.blocks=24
datacloud.match.num.threads=8
datacloud.match.bulk.group.size=20
datacloud.match.realtime.group.size=20
datacloud.match.num.fetchers=32
datacloud.match.num.slowfetchers=8
datacloud.match.realtime.fetchers.enable=true
datacloud.match.cascading.partitions=8
datacloud.match.cascading.rows.threshold=100000000
datacloud.match.public_domain.path=/Pods/Production/Services/PropData/MatchService/PublicDomain/*.avro
datacloud.match.retention.days=7
datacloud.match.bulk.snappy.compress=true

datacloud.match.latest.data.cloud.major.version=2.0
datacloud.match.default.decision.graph=Pokemon

datacloud.match.dnbLookupActor.actor.cardinality=3
datacloud.match.dnbCacheLookupActor.actor.cardinality=8
datacloud.match.dynamoLookupActor.actor.cardinality=16
datacloud.match.dunsGuideBookLookupActor.actor.cardinality=8
datacloud.match.metricActor.actor.cardinality=4

datacloud.match.actor.datasource.default.threadpool.count.min=16
datacloud.match.actor.datasource.default.threadpool.count.max=64
datacloud.match.actor.datasource.default.threadpool.queue.size=1024

datacloud.match.actor.datasource.dnb.threadpool.count.min=2
datacloud.match.actor.datasource.dnb.threadpool.count.max=8
datacloud.match.actor.datasource.dnb.api.call.maxwait=90

datacloud.match.dynamo.fetchers.num=32
datacloud.match.num.dynamo.fetchers.batch.num=32
datacloud.match.dynamo.fetchers.chunk.size=25

datacloud.match.publish.match.history=true

datacloud.yarn.container.mem.mb=8192
datacloud.yarn.container.vcores=4
datacloud.yarn.container.mem.mb.actors=32768
datacloud.yarn.container.vcores.actors=8
datacloud.yarn.actors.num.threads=256
datacloud.yarn.actors.group.size=16

datacloud.ga.username=propdata-service@lattice-engines.com
datacloud.ga.password.hash=EETAlfvFzCdm6/t3Ro8g89vzZo6EDCbucJMTPhYgWiE=
datacloud.ga.password.encrypted=8WurN7eex7f2oY01mZ9C0w==

datacloud.source.db.json=source_dbs_prod.json
datacloud.target.db.json=source_dbs_prod.json

datacloud.core.accountmasterstats.locationbased=false

#collector
datacloud.collector.enabled=true

# etl
datacloud.collection.host=bodcprodvsql230.prod.lattice.local
datacloud.collection.port=1433
datacloud.collection.db=CollectionDB

datacloud.test.host=bodcprodvsql230.prod.lattice.local
datacloud.test.port=1433
datacloud.test.db=LDC_TestDB

datacloud.bulk.host=bodcprodvsql228.prod.lattice.local
datacloud.bulk.port=1433
datacloud.bulk.db=LEDataDB_30

datacloud.user=DLTransfer
datacloud.password.encrypted=Q1nh4HIYGkg4OnQIEbEuiw==

datacloud.collection.sqoop.mapper.number=16
datacloud.collection.cascading.partitions=32
datacloud.collection.cascading.platform=tez

datacloud.etl.cascading.platform=tez
datacloud.etl.cascading.partitions=72
datacloud.etl.workflow.mem.mb=4096
datacloud.etl.cascading.tez.task.mem.gb=4
datacloud.etl.cascading.tez.task.mem.vcores=1
datacloud.etl.hive.enabled=true
datacloud.etl.profile.encode.bit=64
datacloud.etl.profile.attrs=1000
datacloud.etl.am.max.decode.num=100

datacloud.slack.webhook.url=https://hooks.slack.com/services/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

#dnb
datacloud.dnb.use.remote.global=true
datacloud.dnb.bulk.api.key=P10000049C6AC8A0D4743FF86CAE9F6E
datacloud.dnb.realtime.api.key=P100000B35EE924CE024DA5B70BD55D1
datacloud.dnb.bulk.password.encrypted=1wNHSttB4lCZ9tueZRlIAw==
datacloud.dnb.realtime.password.encrypted=1wNHSttB4lCZ9tueZRlIAw==
datacloud.dnb.user.header=x-dnb-user
datacloud.dnb.password.header=x-dnb-pwd
datacloud.dnb.authority.url=https://direct.dnb.com/Authentication/V2.0/
datacloud.dnb.application.id.header=ApplicationId
datacloud.dnb.application.id=37
datacloud.dnb.authorization.header=Authorization
datacloud.dnb.token.cache.expiration.duration.minute=20
datacloud.dnb.authentication.token.jsonpath=$.AuthenticationDetail.Token
datacloud.dnb.dispatcher.frequency.sec=30
datacloud.dnb.status.frequency.sec=120
datacloud.dnb.batch.fetcher.num=4
datacloud.dnb.confidencecode.threshold=6
datacloud.dnb.retry.maxattempts=10

#dnb cache
datacloud.dnb.cache.version=2.0.1
datacloud.dnb.cache.white.expire.days=180
datacloud.dnb.cache.black.expire.days=30
datacloud.dnb.cache.notinam.expire.days=10
datacloud.dnb.cache.expire.factor=0.1
datacloud.dnb.cache.queue.size=50000

#dnb real time
datacloud.dnb.realtime.url.prefix=https://direct.dnb.com/V5.0/organizations?
datacloud.dnb.realtime.email.lookup.url.format=https://direct.dnb.com/V6.3/organizations?findcontact=true&ContactEmailAddress=%s&SearchModeDescription=EmailLookup
datacloud.dnb.realtime.duns.jsonpath=$.GetCleanseMatchResponse.GetCleanseMatchResponseDetail.MatchResponseDetail.MatchCandidate[0].DUNSNumber
datacloud.dnb.realtime.name.jsonpath=$.GetCleanseMatchResponse.GetCleanseMatchResponseDetail.MatchResponseDetail.MatchCandidate[0].OrganizationPrimaryName.OrganizationName.$
datacloud.dnb.realtime.street.jsonpath=$.GetCleanseMatchResponse.GetCleanseMatchResponseDetail.MatchResponseDetail.MatchCandidate[0].PrimaryAddress.StreetAddressLine[0].LineText
datacloud.dnb.realtime.city.jsonpath=$.GetCleanseMatchResponse.GetCleanseMatchResponseDetail.MatchResponseDetail.MatchCandidate[0].PrimaryAddress.PrimaryTownName
datacloud.dnb.realtime.state.jsonpath=$.GetCleanseMatchResponse.GetCleanseMatchResponseDetail.MatchResponseDetail.MatchCandidate[0].PrimaryAddress.TerritoryAbbreviatedName
datacloud.dnb.realtime.countrycode.jsonpath=$.GetCleanseMatchResponse.GetCleanseMatchResponseDetail.MatchResponseDetail.MatchCandidate[0].PrimaryAddress.CountryISOAlpha2Code
datacloud.dnb.realtime.zipcode.jsonpath=$.GetCleanseMatchResponse.GetCleanseMatchResponseDetail.MatchResponseDetail.MatchCandidate[0].PrimaryAddress.PostalCode
datacloud.dnb.realtime.phonenumber.jsonpath=$.GetCleanseMatchResponse.GetCleanseMatchResponseDetail.MatchResponseDetail.MatchCandidate[0].TelephoneNumber.TelecommunicationNumber
datacloud.dnb.realtime.confidencecode.jsonpath=$.GetCleanseMatchResponse.GetCleanseMatchResponseDetail.MatchResponseDetail.MatchCandidate[0].MatchQualityInformation.ConfidenceCodeValue
datacloud.dnb.realtime.matchgrade.jsonpath=$.GetCleanseMatchResponse.GetCleanseMatchResponseDetail.MatchResponseDetail.MatchCandidate[0].MatchQualityInformation.MatchGradeText
datacloud.dnb.realtime.operatingstatus.jsonpath=$.GetCleanseMatchResponse.GetCleanseMatchResponseDetail.MatchResponseDetail.MatchCandidate[0].OperatingStatusText.$
datacloud.dnb.realtime.operatingstatus.outofbusiness=Out of Business
datacloud.dnb.realtime.email.duns.jsonpath=$.FindContactResponse.FindContactResponseDetail.FindCandidate[0].DUNSNumber
datacloud.dnb.realtime.resultid.jsonpath=$..TransactionResult.ResultID
datacloud.dnb.realtime.reasoncode.de=6332
datacould.dnb.realtime.timeout.minute=10

#dnb bulk lookup
datacloud.dnb.bulk.url=https://direct.dnb.com:8443/V3.0/Batches
datacloud.dnb.bulk.servicebatchid.xpath=//ServiceBatchID
datacloud.dnb.bulk.receive.timestamp.xpath=//BatchReceivedTimeStamp
datacloud.dnb.bulk.complete.timestamp.xpath=//ActualProcessCompletionTimestamp
datacloud.dnb.bulk.output.content.object.xpath=//GetBatchResultsResponseDetail/OutputDetail/OutputObjectDetail[%s]/ContentObject
datacloud.dnb.bulk.result.id.xpath=//GetBatchResultsResponseDetail/BatchResult/ResultID
datacloud.dnb.bulk.input.record.format=,"%s",,,,,,,,"%s",,,,"%s","%s","%s",,"%s",,"%s",,
datacloud.dnb.bulk.getresult.url.format=https://direct.dnb.com:8443/V3.0/Batches/%s?SubmittingOfficeID=%s&ServiceVersionNumber=%s&ApplicationTransactionID=123456789&TransactionTimestamp=%s
datacloud.dnb.bulk.office.id=333
datacloud.dnb.bulk.service.number=3
datacloud.dnb.bulk.timeout.minute=180
datacloud.dnb.bulk.request.maximum=10000
datacloud.dnb.bulk.requests.per.second=5
datacloud.dnb.bulk.requests.per.hour=300
datacloud.dnb.bulk.rows.per.hour=500000
datacloud.dnb.bulk.status.request.per.second=5
datacloud.dnb.bulk.status.request.per.hour=300
datacloud.dnb.bulk.retry.times=1
datacloud.dnb.bulk.retry.wait.minute=45
datacloud.dnb.bulk.retry.pendingrecord.threshold=100000
datacloud.dnb.bulk.redirect.realtime.threshold=100

# dnb bulk status lookup
datacloud.dnb.bulk.getstatus.url.format=https://direct.dnb.com:8443/V1.0/Batches?ServiceBatchID=%s&SubmittingOfficeID=333&ServiceVersionNumber=3&ApplicationTransactionID=123456789&TransactionTimestamp=%s&ServiceBatchID-2=1018
datacloud.dnb.bulk.getstatus.batchsize=50
datacloud.dnb.bulk.getstatus.transactioncode.xpath=//TransactionResult/ResultID
datacloud.dnb.bulk.getstatus.servicebatchid.xpath=//ListBatchResponseDetail/Batch[%s]/BatchDetail/ServiceBatchID
datacloud.dnb.bulk.getstatus.status.xpath=//ListBatchResponseDetail/Batch[%s]/BatchResult/ResultID

# aws creds
datacloud.aws.qa.access.key=bi0mpJJNxiYpEka5C6JO4oHwKjKDDkN1OtGcSx6m3TOO20J0JJzWTzNWB0kOXR4yM8mGlBYofKHAUPsb0U92MA==
datacloud.aws.qa.secret.key=bi0mpJJNxiYpEka5C6JO4uuyFoO8IiKpLn5ZS/LmNslND9h+pMMRbh58ZB/qVA0jn/08q5MnTQjhcH9bt9zwsBvRTvUEOR1YcOPWgg8Cs24=
datacloud.aws.prod.access.key=bi0mpJJNxiYpEka5C6JO4iZCri9OL8Z+xrD9An4YnSi3eRDEDU6XCPYmGsB9SxsfDvcaF1rj/gF+PsqrcXGRhA==
datacloud.aws.prod.secret.key=bi0mpJJNxiYpEka5C6JO4k6DQFG6dKs3IRJglYQq5x72PnN/w4zi2cmTvgAqgol4QUZX0pFov2GRYTLkAbKycUtLWZJFjmfZRd78uDYw9Ns=

#Madison Logic
propdata.jobs.enabled=true
propdata.basedir=/user/propdata
propdata.data.source.dir=madison

propdata.madison.datasource.dialect=org.hibernate.dialect.SQLServerDialect
propdata.madison.datasource.driver=com.microsoft.sqlserver.jdbc.SQLServerDriver
propdata.madison.datasource.url=jdbc:sqlserver://bodcprodvsql244.prod.lattice.local;databaseName=MadisonLogic;
propdata.madison.datasource.user=DLTransfer
propdata.madison.datasource.password.encrypted=Q1nh4HIYGkg4OnQIEbEuiw==
propdata.madison.mapper.number=16
propdata.madison.split.columns=ID
propdata.madison.num.past.days=28
propdata.madison.datasource.data.url=jdbc:sqlserver://bodcprodvsql244.prod.lattice.local;databaseName=MadisonLogic;
propdata.madison.datasource.data.host=bodcprodvsql244.prod.lattice.local
propdata.madison.datasource.data.port=1433
propdata.madison.datasource.data.dbname=MadisonLogic
propdata.madison.datasource.data.type=SQLServer
propdata.madison.datasource.data.user=DLTransfer
propdata.madison.datasource.data.password.encrypted=Q1nh4HIYGkg4OnQIEbEuiw==

propdata.madison.target.raw.table=MadisonLogicDepivoted
propdata.madison.target.table=MadisonLogicAggregated_Source
propdata.madison.datatarget.url=jdbc:sqlserver://bodcprodvsql230.prod.lattice.local;databaseName=CollectionDB;
propdata.madison.datatarget.host=bodcprodvsql230.prod.lattice.local
propdata.madison.datatarget.port=1433
propdata.madison.datatarget.dbname=CollectionDB
propdata.madison.datatarget.type=SQLServer
propdata.madison.datatarget.user=DLTransfer
propdata.madison.datatarget.password.encrypted=Q1nh4HIYGkg4OnQIEbEuiw==

propdata.madison.download.schedule=0 0 * * * ?
propdata.madison.upload.schedule=0 0 2/7 * * ?
propdata.madison.use.default.job.properties=false
propdata.madison.cascading.engine=tez

# test
datacloud.test.env=prodcluster
datacloud.test.match.client=PD131
datacloud.test.match.correctness.rows=1000

# collection
datacloud.collection.s3bucket=latticeengines-prod-datacloud
datacloud.collection.s3bucket.prefix=collectors/workers/

datacloud.collection.ecr.image.name=latticeengines/collector
datacloud.collection.ecs.cluster.name=datacloud-collector
datacloud.collection.ecs.task.def.name=datacloud-collector-task-def
datacloud.collection.ecs.task.cpu=512
datacloud.collection.ecs.task.memory=1024
datacloud.collection.ecs.task.subnets=subnet-87cdd6ad,subnet-b80a12e0,subnet-ad7db6e4


#collection test
datacloud.collection.test.domains=

#ingestion of collected data
datacloud.collection.bw.timestamp=CollectedAt
#3600 second
datacloud.collection.ingestion.partion.period=3600
# ==================================================
# le-metadata/conf/env/prodcluster/metadata.properties
# ==================================================
metadata.hive.enabled=false
metadata.hive.url=jdbc:hive2://bodcprodvhdp50.prod.lattice.local:10015
metadata.hive.username=yarn
metadata.hive.password.encrypted=

# ==================================================
# le-scoringapi/conf/env/prodcluster/scoringapi.properties
# ==================================================
scoringapi.scoreartifact.cache.maxsize=80
scoringapi.scoreartifact.cache.default.size=2097152
scoringapi.scoreartifact.cache.max.threshold=0.5
scoringapi.scoreartifact.cache.max.weight=8589934592
scoringapi.scoreartifact.cache.ratio=60
scoringapi.scoreartifact.cache.concurrency.level=1
scoringapi.scoreartifact.cache.expiration.time=1
scoringapi.scoreartifact.cache.refresh.time=120

scoringapi.modeldetailsandfields.cache.maxsize=500
scoringapi.modeldetailsandfields.cache.expiration.time=1

scoringapi.enrichment.cache.size=100
scoringapi.enrichment.cache.expiration.time=5

scoringapi.propdata.shortcircuit=false
scoringapi.ratelimit.max=100
scoringapi.ratelimit.bulk.requests.max=25
scoringapi.ratelimit.single.requests.max=100

scoringapi.jythonengine.cache.maxsize=200
scoringapi.score.history.publish.enabled=true

scoringapi.modeljson.cache.dir=/var/cache/scoringapi/%s/%s/%s/

