{"cells":[{"cell_type":"code","execution_count":110,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Ready? Go!Scala library version 2.11.12 -- Copyright 2002-2017, LAMP/EPFL\n"}],"source":["// do anything to start a livy session\n","print(\"Ready? Go!\")\n","\n","println(util.Properties.versionMsg)"]},{"cell_type":"code","execution_count":111,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Starting Spark application\n"},{"data":{"text/html":"<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>654</td><td>application_1581906474475_108484</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-141-11-186.lattice.local:20888/proxy/application_1581906474475_108484/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-141-11-68.lattice.local:8042/node/containerlogs/container_1581906474475_108484_01_000001/livy\">Link</a></td><td>✔</td></tr></table>","text/plain":"<IPython.core.display.HTML object>"},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":"SparkSession available as 'spark'.\n"},{"data":{"text/html":"Current session configs: <tt>{'conf': {'spark.hadoop.hadoop.security.credential.provider.path': 'jceks://hdfs/user/root/awsqakeyfile.jceks', 'spark.driver.maxResultSize': '1g', 'spark.driver.memory': '16g', 'spark.executor.cores': '5', 'spark.executor.memory': '16g', 'spark.dynamicAllocation.maxExecutors': '16'}, 'proxyUser': 'jovyan', 'kind': 'spark'}</tt><br>","text/plain":"<IPython.core.display.HTML object>"},"metadata":{},"output_type":"display_data"},{"data":{"text/html":"<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>654</td><td>application_1581906474475_108484</td><td>spark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-10-141-11-186.lattice.local:20888/proxy/application_1581906474475_108484/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-10-141-11-68.lattice.local:8042/node/containerlogs/container_1581906474475_108484_01_000001/livy\">Link</a></td><td>✔</td></tr></table>","text/plain":"<IPython.core.display.HTML object>"},"metadata":{},"output_type":"display_data"}],"source":["%%configure -f\n","{\n","    \"conf\": {\n","        \"spark.hadoop.hadoop.security.credential.provider.path\": \"jceks://hdfs/user/root/awsqakeyfile.jceks\",\n","        \"spark.driver.maxResultSize\": \"1g\",\n","        \"spark.driver.memory\": \"16g\",\n","        \"spark.executor.cores\": \"5\",\n","        \"spark.executor.memory\": \"16g\",\n","        \"spark.dynamicAllocation.maxExecutors\": \"16\"\n","    }\n","}"]},{"cell_type":"code","execution_count":112,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"import com.fasterxml.jackson.databind.{JsonNode, ObjectMapper}\nimport com.fasterxml.jackson.module.scala.DefaultScalaModule\nimport com.fasterxml.jackson.module.scala.experimental.ScalaObjectMapper\nimport org.apache.spark.sql.{DataFrame, SparkSession}\nspark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@3937e276\ncheckpointDir: String = \"\"\n"}],"source":["import com.fasterxml.jackson.databind.{JsonNode, ObjectMapper}\n","import com.fasterxml.jackson.module.scala.DefaultScalaModule\n","import com.fasterxml.jackson.module.scala.experimental.ScalaObjectMapper\n","import org.apache.spark.sql.{DataFrame, SparkSession}\n","\n","val spark = SparkSession.builder().appName(\"SparkSession\").getOrCreate()\n","val checkpointDir = \"\"\n","\n","// rememer cleanup checkpoint dir regularly\n","if (checkpointDir.length > 0) {\n","  spark.sparkContext.setCheckpointDir(checkpointDir)\n","}"]},{"cell_type":"markdown","metadata":{},"source":["### Read"]},{"cell_type":"code","execution_count":113,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"import org.apache.spark.sql.functions._\nimport org.apache.spark.sql.SaveMode\nTENANT: String = slin_ga_test_04\nSYS_ACC_ID: String = user_DefaultSystem_5nzu3m1p_AccountId\nSYS_CONTACT_ID: String = user_DefaultSystem_j1wuba21_ContactId\nHDFS_ROOT: String = hdfs:///Pods/QA/Contracts/slin_ga_test_04/Tenants/slin_ga_test_04/Spaces/Production/Data/Tables/\nS3_ROOT: String = s3a://latticeengines-qa-customers/slin_ga_test_04/atlas/Data/Tables/\ncopySuffixFn: (org.apache.spark.sql.DataFrame, Seq[String], String, String) => (org.apache.spark.sql.DataFrame, Boolean) = <function4>\n"}],"source":["import org.apache.spark.sql.functions._\n","import org.apache.spark.sql.SaveMode\n","\n","/*\n"," * configure tenant & ids\n"," */\n","val TENANT = \"slin_ga_test_04\"\n","val SYS_ACC_ID = \"user_DefaultSystem_5nzu3m1p_AccountId\"\n","val SYS_CONTACT_ID = \"user_DefaultSystem_j1wuba21_ContactId\"\n","\n","/*\n"," * configure hdfs/s3 root path\n"," */\n","val HDFS_ROOT = s\"hdfs:///Pods/QA/Contracts/$TENANT/Tenants/$TENANT/Spaces/Production/Data/Tables/\"\n","val S3_ROOT = s\"s3a://latticeengines-qa-customers/$TENANT/atlas/Data/Tables/\"\n","\n","val copySuffixFn = (df: DataFrame, cols: Seq[String], srcSuffix: String, tgtSuffix: String) => {\n","    var changed = false\n","    var res = df\n","    cols.filter(_.endsWith(srcSuffix)) foreach { id =>\n","        val dst = id.replace(srcSuffix, tgtSuffix)\n","        if (cols.contains(dst)) {\n","            println(s\"=== skip copying [$id] to [$dst], target attr already exist in table\")\n","        } else {\n","            changed = true\n","            res = res.withColumn(dst, res.col(id))\n","            println(s\"=== copy attribute [$id] to [$dst]\")\n","        }\n","    }\n","    (res, changed)\n","}"]},{"cell_type":"code","execution_count":81,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"s3Path: String = s3a://latticeengines-qa-customers/slin_ga_test_03/atlas/Data/Tables/Account_2020-03-09_07-27-59_UTC/*.avro\naccounts: org.apache.spark.sql.DataFrame = [EntityId: string, CustomerAccountId: string ... 29 more fields]\n\n===test=== 900\n"}],"source":["val s3Path = S3_ROOT + \"Account_2020-03-09_07-27-59_UTC/*.avro\"\n","val accounts = spark.read.format(\"avro\").load(s3Path)\n","\n","// val dedupAccs = accounts.dropDuplicates(\"EntityId\")\n","// dedupAccs.write.format(\"avro\").mode(\"Overwrite\").save(S3_ROOT + \"Account_2020-03-09_07-27-59_UTC\")\n","println(\"\\n===test=== \" + accounts.count)"]},{"cell_type":"code","execution_count":114,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"accBatchTable: String = Account_2020-03-10_06-55-23_UTC\ns3Path: String = s3a://latticeengines-qa-customers/slin_ga_test_04/atlas/Data/Tables/Account_2020-03-10_06-55-23_UTC\naccounts: org.apache.spark.sql.DataFrame = [EntityId: string, CustomerAccountId: string ... 29 more fields]\n\n=== Total Records Before: 900\n=== Cols Before: 31\ncols: Seq[String] = WrappedArray(EntityId, CustomerAccountId, Industry, SpendAnalyticsSegment, Website, CompanyName, City, State, Country, PostalCode, user_fix_du_tree_flag, user_street, user_sales_in__b, user_total_employees, user_ceo_1st_name, user_ceo_last_name, user_ceo_title, user_salesforceaccountid, user_salesforcesandboxaccountid, user_marketoaccountid, user_strange_attr, user_overlap_attr, user_test_date, user_test_date_2, user_test_date_3, user_test_date_4, InternalId, AccountId, LatticeAccountId, CDLCreatedTime, CDLUpdatedTime)\nno system account id user_DefaultSystem_5nzu3m1p_AccountId found in path s3a://latticeengines-qa-customers/slin_ga_test_04/atlas/Data/Tables/Account_2020-03-10_06-55-23_UTC\ncols after change: WrappedArray(EntityId, CustomerAccountId, Industry, SpendAnalyticsSegment, Website, CompanyName, City, State, Country, PostalCode, user_fix_du_tree_flag, user_street, user_sales_in__b, user_total_employees, user_ceo_1st_name, user_ceo_last_name, user_ceo_title, user_salesforceaccountid, user_salesforcesandboxaccountid, user_marketoaccountid, user_strange_attr, user_overlap_attr, user_test_date, user_test_date_2, user_test_date_3, user_test_date_4, InternalId, AccountId, LatticeAccountId, CDLCreatedTime, CDLUpdatedTime, user_DefaultSystem_5nzu3m1p_AccountId)\n[109,109,Electronics,General Practice,amphenol.com,Amphenol,Wallingford,Connecticut,United States,06492-3574,N,358 Hall Avenue,$5.69,50700,Richard,Norwitt,Chief Executive Officer,109,1ba8c66c-7c47-45f1-be65-6f1811052f26,526f5cf7-3eca-4bbc-89aa-496358d211b3,Nulla neque libero, convallis eget, eleifend luctus, ultricies eu, nibh. Quisque id justo sit amet sapien dignissim vestibulum. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; Nulla dapibus dolor vel est. Donec odio justo, sollicitudin ut, suscipit a, feugiat et, eros. Vestibulum ac est lacinia nisi venenatis tristique. Fusce congue, diam id ornare imperdiet, sapien urna pretium nisl, ut volutpat sapien arcu sed augue.,value_1,1505433600000,1502409600000,1501508570000,1503457871000,110,109,0310001500442,1583823358915,1583823358915,109]\n\n=== Total Records After: 900\n=== Cols After: 32, WrappedArray(EntityId, CustomerAccountId, Industry, SpendAnalyticsSegment, Website, CompanyName, City, State, Country, PostalCode, user_fix_du_tree_flag, user_street, user_sales_in__b, user_total_employees, user_ceo_1st_name, user_ceo_last_name, user_ceo_title, user_salesforceaccountid, user_salesforcesandboxaccountid, user_marketoaccountid, user_strange_attr, user_overlap_attr, user_test_date, user_test_date_2, user_test_date_3, user_test_date_4, InternalId, AccountId, LatticeAccountId, CDLCreatedTime, CDLUpdatedTime, user_DefaultSystem_5nzu3m1p_AccountId)\n"}],"source":["/*\n"," * account batch store (configure table name)\n"," */\n","val accBatchTable = \"Account_2020-03-10_06-55-23_UTC\"\n","val s3Path = S3_ROOT + accBatchTable\n","val accounts = spark.read.format(\"avro\").load(s3Path + \"/*.avro\")\n","println(\"\\n=== Total Records Before: \" + accounts.count)\n","println(\"=== Cols Before: \" + accounts.columns.toSeq.size)\n","val cols = accounts.columns.toSeq\n","if (!cols.contains(SYS_ACC_ID)) {\n","    println(s\"no system account id $SYS_ACC_ID found in path $s3Path\")\n","    val accountsWithId = accounts.withColumn(SYS_ACC_ID, accounts.col(\"CustomerAccountId\"))\n","    println(\"cols after change: \" + accountsWithId.columns.toSeq)\n","    println(accountsWithId.take(1).head)\n","    println(\"\\n=== Total Records After: \" + accounts.count)\n","    println(\"=== Cols After: \" + accountsWithId.columns.toSeq.size + \", \" + accountsWithId.columns.toSeq)\n","    accountsWithId.write.format(\"avro\").mode(\"Overwrite\").save(s3Path + \"-fix\")\n","} else {\n","    println(s\"=== no change\")\n","}"]},{"cell_type":"code","execution_count":115,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"accSysTable: String = SystemAccount_2020-03-10_06-55-23_UTC\naccSysPath: String = s3a://latticeengines-qa-customers/slin_ga_test_04/atlas/Data/Tables/SystemAccount_2020-03-10_06-55-23_UTC\naccSys: org.apache.spark.sql.DataFrame = [SourceFile_file_1583820472295_csv__CustomerAccountId: string, SourceFile_file_1583820472295_csv__Industry: string ... 30 more fields]\ncols: Seq[String] = WrappedArray(SourceFile_file_1583820472295_csv__CustomerAccountId, SourceFile_file_1583820472295_csv__Industry, SourceFile_file_1583820472295_csv__SpendAnalyticsSegment, SourceFile_file_1583820472295_csv__Website, SourceFile_file_1583820472295_csv__CompanyName, SourceFile_file_1583820472295_csv__City, SourceFile_file_1583820472295_csv__State, SourceFile_file_1583820472295_csv__Country, SourceFile_file_1583820472295_csv__PostalCode, SourceFile_file_1583820472295_csv__user_fix_du_tree_flag, SourceFile_file_1583820472295_csv__user_street, SourceFile_file_1583820472295_csv__user_sales_in__b, SourceFile_file_1583820472295_csv__user_total_employees, SourceFile_file_1583820472295_csv__user_ceo_1st_name, SourceFile_file_1583820472295_csv__user_ceo_last_name, SourceFile_file_...\n=== Total Records Before: 900\n=== Cols Before: 32\n=== copy attribute [SourceFile_file_1583820472295_csv__CustomerAccountId] to [SourceFile_file_1583820472295_csv__user_DefaultSystem_5nzu3m1p_AccountId]\nresult: (org.apache.spark.sql.DataFrame, Boolean) = ([SourceFile_file_1583820472295_csv__CustomerAccountId: string, SourceFile_file_1583820472295_csv__Industry: string ... 31 more fields],true)\nsuffixes: Seq[String] = List(CustomerAccountId, user_DefaultSystem_5nzu3m1p_AccountId)\n\n=== cols after change: WrappedArray(SourceFile_file_1583820472295_csv__CustomerAccountId, SourceFile_file_1583820472295_csv__user_DefaultSystem_5nzu3m1p_AccountId)\n=== Total Records After: 900\n=== Cols After: 33, WrappedArray(SourceFile_file_1583820472295_csv__CustomerAccountId, SourceFile_file_1583820472295_csv__Industry, SourceFile_file_1583820472295_csv__SpendAnalyticsSegment, SourceFile_file_1583820472295_csv__Website, SourceFile_file_1583820472295_csv__CompanyName, SourceFile_file_1583820472295_csv__City, SourceFile_file_1583820472295_csv__State, SourceFile_file_1583820472295_csv__Country, SourceFile_file_1583820472295_csv__PostalCode, SourceFile_file_1583820472295_csv__user_fix_du_tree_flag, SourceFile_file_1583820472295_csv__user_street, SourceFile_file_1583820472295_csv__user_sales_in__b, SourceFile_file_1583820472295_csv__user_total_employees, SourceFile_file_1583820472295_csv__user_ceo_1st_name, SourceFile_file_1583820472295_csv__user_ceo_last_name, SourceFile_file_1583820472295_csv__user_ceo_title, SourceFile_file_1583820472295_csv__user_salesforceaccountid, SourceFile_file_1583820472295_csv__user_salesforcesandboxaccountid, SourceFile_file_1583820472295_csv__user_marketoaccountid, SourceFile_file_1583820472295_csv__user_strange_attr, SourceFile_file_1583820472295_csv__user_overlap_attr, SourceFile_file_1583820472295_csv__user_test_date, SourceFile_file_1583820472295_csv__user_test_date_2, SourceFile_file_1583820472295_csv__user_test_date_3, SourceFile_file_1583820472295_csv__user_test_date_4, SourceFile_file_1583820472295_csv__InternalId, EntityId, SourceFile_file_1583820472295_csv__AccountId, SourceFile_file_1583820472295_csv__LatticeAccountId, SourceFile_file_1583820472295_csv__CDLCreatedTime, SourceFile_file_1583820472295_csv__CDLUpdatedTime, SourceFile_file_1583820472295_csv__CDLBatchSource, SourceFile_file_1583820472295_csv__user_DefaultSystem_5nzu3m1p_AccountId)\n"}],"source":["/*\n"," * account system batch store (configure table name)\n"," */\n","val accSysTable = \"SystemAccount_2020-03-10_06-55-23_UTC\"\n","val accSysPath = S3_ROOT + accSysTable\n","val accSys = spark.read.format(\"avro\").load(accSysPath + \"/*.avro\")\n","val cols = accSys.columns.toSeq\n","\n","println(\"\\n=== Total Records Before: \" + accSys.count)\n","println(\"=== Cols Before: \" + accSys.columns.toSeq.size)\n","\n","val result = copySuffixFn(accSys, cols, \"CustomerAccountId\", SYS_ACC_ID)\n","\n","val suffixes = Seq(\"CustomerAccountId\", SYS_ACC_ID)\n","\n","if (result._2) {\n","    println(\"\\n=== cols after change: \" + result._1.columns.toSeq\n","            .filter(col => suffixes.foldLeft(false)((acc, s) => acc || col.endsWith(s))))\n","    println(\"=== Total Records After: \" + result._1.count)\n","    println(\"=== Cols After: \" + result._1.columns.toSeq.size + \", \" + result._1.columns.toSeq)\n","    result._1.write.format(\"avro\").mode(\"Overwrite\").save(accSysPath + \"-fix\")\n","} else {\n","    println(\"=== no change\")\n","}"]},{"cell_type":"markdown","metadata":{},"source":["### Compute"]},{"cell_type":"markdown","metadata":{},"source":["### Write"]},{"cell_type":"code","execution_count":118,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"contactBatchStore: String = Contact_2020-03-10_08-26-38_UTC\ns3Path: String = s3a://latticeengines-qa-customers/slin_ga_test_04/atlas/Data/Tables/Contact_2020-03-10_08-26-38_UTC\ncontacts: org.apache.spark.sql.DataFrame = [EntityId: string, CustomerContactId: string ... 25 more fields]\ncols: Seq[String] = WrappedArray(EntityId, CustomerContactId, ContactName, FirstName, LastName, CustomerAccountId, Title, NumberOfEmployees, Industry, DoNotMail, CreatedDate, Email, PostalCode, Address_Street_1, user_gender, user_ip_address, user_some_description, user_last_communication_date, user_renewal_date, InternalId, ContactId, AccountId, LatticeAccountId, CDLCreatedTime, CDLUpdatedTime, user_DefaultSystem_5nzu3m1p_AccountId, user_DefaultSystem_j1wuba21_ContactId)\n\n=== Total Records Before: 900\n=== Cols Before: 27, WrappedArray(EntityId, CustomerContactId, ContactName, FirstName, LastName, CustomerAccountId, Title, NumberOfEmployees, Industry, DoNotMail, CreatedDate, Email, PostalCode, Address_Street_1, user_gender, user_ip_address, user_some_description, user_last_communication_date, user_renewal_date, InternalId, ContactId, AccountId, LatticeAccountId, CDLCreatedTime, CDLUpdatedTime, user_DefaultSystem_5nzu3m1p_AccountId, user_DefaultSystem_j1wuba21_ContactId)\n=== no change\n"}],"source":["/*\n"," * contact batch store (configure table name)\n"," */\n","val contactBatchStore = \"Contact_2020-03-10_08-26-38_UTC\"\n","val s3Path = S3_ROOT + contactBatchStore\n","val contacts = spark.read.format(\"avro\").load(s3Path + \"/*.avro\")\n","val cols = contacts.columns.toSeq\n","\n","println(\"\\n=== Total Records Before: \" + contacts.count)\n","println(\"=== Cols Before: \" + contacts.columns.toSeq.size + \", \" + contacts.columns.toSeq)\n","\n","if (!cols.contains(SYS_ACC_ID) && !cols.contains(SYS_CONTACT_ID)) {\n","    println(s\"no system account id [$SYS_ACC_ID] and system contact id [$SYS_CONTACT_ID] found in path $s3Path\")\n","    val contactsWithId = contacts\n","        .withColumn(SYS_ACC_ID, contacts.col(\"CustomerAccountId\"))\n","        .withColumn(SYS_CONTACT_ID, contacts.col(\"CustomerContactId\"))\n","    println(\"=== cols after change: \" + contactsWithId.columns.toSeq.filter(col => col.equals(SYS_ACC_ID) || col.endsWith(SYS_CONTACT_ID)))\n","    println(\"\\n=== Total Records After: \" + contacts.count)\n","    println(\"=== Cols After: \" + contactsWithId.columns.toSeq.size + \", \" + contactsWithId.columns.toSeq)\n","    contactsWithId.write.format(\"avro\").mode(\"Overwrite\").save(s3Path + \"-fix\")\n","} else {\n","    println(\"=== no change\")\n","}"]},{"cell_type":"code","execution_count":117,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"contactSysTable: String = SystemContact_2020-03-10_08-26-38_UTC\ns3Path: String = s3a://latticeengines-qa-customers/slin_ga_test_04/atlas/Data/Tables/SystemContact_2020-03-10_08-26-38_UTC\ncontacts: org.apache.spark.sql.DataFrame = [SourceFile_file_1583820509665_csv__CustomerContactId: string, SourceFile_file_1583820509665_csv__ContactName: string ... 24 more fields]\ncols: Seq[String] = WrappedArray(SourceFile_file_1583820509665_csv__CustomerContactId, SourceFile_file_1583820509665_csv__ContactName, SourceFile_file_1583820509665_csv__FirstName, SourceFile_file_1583820509665_csv__LastName, SourceFile_file_1583820509665_csv__CustomerAccountId, SourceFile_file_1583820509665_csv__Title, SourceFile_file_1583820509665_csv__NumberOfEmployees, SourceFile_file_1583820509665_csv__Industry, SourceFile_file_1583820509665_csv__DoNotMail, SourceFile_file_1583820509665_csv__CreatedDate, SourceFile_file_1583820509665_csv__Email, SourceFile_file_1583820509665_csv__PostalCode, SourceFile_file_1583820509665_csv__Address_Street_1, SourceFile_file_1583820509665_csv__user_gender, SourceFile_file_1583820509665_csv__user_ip_address, SourceFile_file_1583820509665_csv__user_...=== WrappedArray(SourceFile_file_1583820509665_csv__CustomerContactId)\n\n=== Total Records Before: 900\n=== Cols Before: 26\n=== copy attribute [SourceFile_file_1583820509665_csv__CustomerAccountId] to [SourceFile_file_1583820509665_csv__user_DefaultSystem_5nzu3m1p_AccountId]\nresultAcc: (org.apache.spark.sql.DataFrame, Boolean) = ([SourceFile_file_1583820509665_csv__CustomerContactId: string, SourceFile_file_1583820509665_csv__ContactName: string ... 25 more fields],true)\n=== copy attribute [SourceFile_file_1583820509665_csv__CustomerContactId] to [SourceFile_file_1583820509665_csv__user_DefaultSystem_j1wuba21_ContactId]\nresult: (org.apache.spark.sql.DataFrame, Boolean) = ([SourceFile_file_1583820509665_csv__CustomerContactId: string, SourceFile_file_1583820509665_csv__ContactName: string ... 26 more fields],true)\nsuffixes: Seq[String] = List(CustomerAccountId, user_DefaultSystem_5nzu3m1p_AccountId, CustomerContactId, user_DefaultSystem_j1wuba21_ContactId)\n=== cols after change: WrappedArray(SourceFile_file_1583820509665_csv__CustomerContactId, SourceFile_file_1583820509665_csv__CustomerAccountId, SourceFile_file_1583820509665_csv__user_DefaultSystem_5nzu3m1p_AccountId, SourceFile_file_1583820509665_csv__user_DefaultSystem_j1wuba21_ContactId)\n\n=== Total Records after: 900\n=== Cols after: 28, WrappedArray(SourceFile_file_1583820509665_csv__CustomerContactId, SourceFile_file_1583820509665_csv__ContactName, SourceFile_file_1583820509665_csv__FirstName, SourceFile_file_1583820509665_csv__LastName, SourceFile_file_1583820509665_csv__CustomerAccountId, SourceFile_file_1583820509665_csv__Title, SourceFile_file_1583820509665_csv__NumberOfEmployees, SourceFile_file_1583820509665_csv__Industry, SourceFile_file_1583820509665_csv__DoNotMail, SourceFile_file_1583820509665_csv__CreatedDate, SourceFile_file_1583820509665_csv__Email, SourceFile_file_1583820509665_csv__PostalCode, SourceFile_file_1583820509665_csv__Address_Street_1, SourceFile_file_1583820509665_csv__user_gender, SourceFile_file_1583820509665_csv__user_ip_address, SourceFile_file_1583820509665_csv__user_some_description, SourceFile_file_1583820509665_csv__user_last_communication_date, SourceFile_file_1583820509665_csv__user_renewal_date, SourceFile_file_1583820509665_csv__InternalId, EntityId, SourceFile_file_1583820509665_csv__ContactId, SourceFile_file_1583820509665_csv__AccountId, SourceFile_file_1583820509665_csv__LatticeAccountId, SourceFile_file_1583820509665_csv__CDLCreatedTime, SourceFile_file_1583820509665_csv__CDLUpdatedTime, SourceFile_file_1583820509665_csv__CDLBatchSource, SourceFile_file_1583820509665_csv__user_DefaultSystem_5nzu3m1p_AccountId, SourceFile_file_1583820509665_csv__user_DefaultSystem_j1wuba21_ContactId)\n"}],"source":["/*\n"," * contact system batch store (configure table name)\n"," */\n","val contactSysTable = \"SystemContact_2020-03-10_08-26-38_UTC\"\n","val s3Path = S3_ROOT + contactSysTable\n","val contacts = spark.read.format(\"avro\").load(s3Path + \"/*.avro\")\n","val cols = contacts.columns.toSeq\n","\n","println(\"=== \" + cols.filter(col => col.endsWith(\"CustomerContactId\")))\n","println(\"\\n=== Total Records Before: \" + contacts.count)\n","println(\"=== Cols Before: \" + contacts.columns.toSeq.size)\n","\n","val resultAcc = copySuffixFn(contacts, cols, \"CustomerAccountId\", SYS_ACC_ID)\n","val result = copySuffixFn(resultAcc._1, cols, \"CustomerContactId\", SYS_CONTACT_ID)\n","\n","val suffixes = Seq(\"CustomerAccountId\", SYS_ACC_ID, \"CustomerContactId\", SYS_CONTACT_ID)\n","\n","if (resultAcc._2 || result._2) {\n","    println(\"=== cols after change: \" + result._1.columns.toSeq\n","            .filter(col => suffixes.foldLeft(false)((acc, s) => acc || col.endsWith(s))))\n","    println(\"\\n=== Total Records after: \" + result._1.count)\n","    println(\"=== Cols after: \" + result._1.columns.toSeq.size + \", \" + result._1.columns.toSeq)\n","    result._1.write.format(\"avro\").mode(\"Overwrite\").save(s3Path + \"-fix\")\n","} else {\n","    println(\"=== no change\")\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"name":"sparkkernel","display_name":"Spark","language":""},"language_info":{"name":"scala","mimetype":"text/x-scala","codemirror_mode":"text/x-scala","pygments_lexer":"scala"}},"nbformat":4,"nbformat_minor":2}